{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97c3653d22990a1a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train set\n",
      "Finished processing valid set\n",
      "Finished processing test set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def parse_annotation(annotation_line):\n",
    "    parts = annotation_line.strip().split()\n",
    "    class_id = int(parts[0])  # The class_id is the first element\n",
    "    vertices = np.array(parts[1:], dtype=np.float32)  # The rest are the vertices\n",
    "    return class_id, vertices.reshape((-1, 2))  # Reshape to Nx2 where N is the number of vertices\n",
    "\n",
    "def draw_polygon_on_mask(mask, corners, image_shape):\n",
    "    scaled_corners = corners * np.array([image_shape[1], image_shape[0]], dtype=np.float32)  # scale x and y\n",
    "    scaled_corners = np.around(scaled_corners).astype(np.int32)  # round and convert to int\n",
    "\n",
    "    # print(\"Scaled Corners:\", scaled_corners)  # Debugging print\n",
    "\n",
    "    corners_int = scaled_corners.reshape((-1, 1, 2))\n",
    "    cv2.fillPoly(mask, [corners_int], color=(255))  # Ensure fillPoly is used, not polylines\n",
    "\n",
    "    # # Debugging visualization\n",
    "    # plt.imshow(mask, cmap='gray')\n",
    "    # plt.title('Polygon on Mask')\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "def create_mask_from_annotations(annotation_path, image_shape):\n",
    "    mask = np.zeros((image_shape[0], image_shape[1]), dtype=np.uint8)  # Create a black mask\n",
    "    with open(annotation_path, 'r') as file:\n",
    "        for line in file:\n",
    "            class_id, vertices = parse_annotation(line)\n",
    "            draw_polygon_on_mask(mask, vertices, image_shape)  # Draw each polygon on the mask\n",
    "    return mask\n",
    "\n",
    "def preprocess_image(image_path, annotation_path, target_size):\n",
    "    # Open and resize image\n",
    "    image = Image.open(image_path).resize(target_size)\n",
    "    mask = create_mask_from_annotations(annotation_path, target_size)  # Note the reversal of width and height for the mask\n",
    "    return np.array(image), mask\n",
    "\n",
    "def process_directory(data_dir, annotation_dir, target_size):\n",
    "    for img_filename in os.listdir(data_dir):\n",
    "        if img_filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(data_dir, img_filename)\n",
    "            annotation_path = os.path.join(annotation_dir, img_filename.replace('.jpg', '.txt'))\n",
    "            image, mask = preprocess_image(image_path, annotation_path, target_size)\n",
    "            np.save(os.path.join(data_dir, img_filename.replace('.jpg', '_img.npy')), image)\n",
    "            np.save(os.path.join(data_dir, img_filename.replace('.jpg', '_mask.npy')), mask)\n",
    "            # plt.imshow(mask, cmap='gray')  # Debugging visualization\n",
    "            # plt.show()\n",
    "            \n",
    "def save_as_npy(data_dir, annotation_dir, target_size=(512, 512)):\n",
    "    processed_data = []\n",
    "    for img_filename in os.listdir(data_dir):\n",
    "        if img_filename.endswith('.jpg'):\n",
    "            image_path = os.path.join(data_dir, img_filename)\n",
    "            annotation_path = os.path.join(annotation_dir, img_filename.replace('.jpg', '.txt'))\n",
    "            image, mask = preprocess_image(image_path, annotation_path, target_size)\n",
    "            np.save(os.path.join(data_dir, img_filename.replace('.jpg', '_img.npy')), image)\n",
    "            np.save(os.path.join(data_dir, img_filename.replace('.jpg', '_mask.npy')), mask)\n",
    "            processed_data.append((image, mask))\n",
    "    print(f\"Processed and saved {len(processed_data)} image-mask pairs in .npy format.\")\n",
    "\n",
    "# Example usage\n",
    "dataset_base = r'C:/Users/SirM/Desktop/Swayam/Intro to Deep Learning/Intro-to-Deep-Learning/Final Project/PKLot.v1-raw.yolov8-obb'\n",
    "partitions = ['train', 'valid', 'test']\n",
    "target_size = (512, 512)  # Change as required by your model\n",
    "\n",
    "# Process images and annotations and save them as .npy files\n",
    "# Process images and annotations\n",
    "for part in partitions:\n",
    "    images_dir = os.path.join(dataset_base, part, 'images')\n",
    "    annotations_dir = os.path.join(dataset_base, part, 'labels')\n",
    "    process_directory(images_dir, annotations_dir, target_size)\n",
    "    print(f\"Finished processing {part} set\")\n",
    "\n",
    "# Visualization (example for one image from the 'train' set)\n",
    "train_images_dir = os.path.join(dataset_base, 'train', 'images')\n",
    "train_image_files = [f for f in os.listdir(train_images_dir) if f.endswith('_img.npy')]\n",
    "\n",
    "# Load one image and its corresponding mask\n",
    "image = np.load(os.path.join(train_images_dir, train_image_files[0]))\n",
    "mask = np.load(os.path.join(train_images_dir, train_image_files[0].replace('_img.npy', '_mask.npy')))\n",
    "\n",
    "# # Visualize the image and the mask\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(image)\n",
    "# plt.title('Processed Image')\n",
    "# plt.axis('off')\n",
    "# \n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(mask, cmap='gray')\n",
    "# plt.title('Processed Mask')\n",
    "# plt.axis('off')\n",
    "# \n",
    "# plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-27T18:51:22.672359600Z",
     "start_time": "2024-04-27T18:45:22.787473700Z"
    }
   },
   "id": "initial_id",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Semantics CNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "201d7c498005250c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8691 images in directory C:/Users/SirM/Desktop/Swayam/Intro to Deep Learning/Intro-to-Deep-Learning/Final Project/PKLot.v1-raw.yolov8-obb\\train\\images\n",
      "Total number of training images: 8691\n",
      "Found 2483 images in directory C:/Users/SirM/Desktop/Swayam/Intro to Deep Learning/Intro-to-Deep-Learning/Final Project/PKLot.v1-raw.yolov8-obb\\valid\\images\n",
      "Found 1242 images in directory C:/Users/SirM/Desktop/Swayam/Intro to Deep Learning/Intro-to-Deep-Learning/Final Project/PKLot.v1-raw.yolov8-obb\\test\\images\n",
      "Epoch 1, Validation Loss: 0.4023, Validation Accuracy: 0.8037\n",
      "Epoch 2, Validation Loss: 0.2826, Validation Accuracy: 0.8802\n",
      "Epoch 3, Validation Loss: 0.2463, Validation Accuracy: 0.8974\n",
      "Epoch 4, Validation Loss: 0.2199, Validation Accuracy: 0.9099\n",
      "Epoch 5, Validation Loss: 0.1967, Validation Accuracy: 0.9196\n",
      "Epoch 6, Validation Loss: 0.1795, Validation Accuracy: 0.9274\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "# Define the CNN architecture for segmentation\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Use sigmoid for binary segmentation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class PKLotDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith('_img.npy')]\n",
    "        self.transform = transform\n",
    "        print(f\"Found {len(self.image_files)} images in directory {data_dir}\")  # Debugging output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        mask_file = image_file.replace('_img.npy', '_mask.npy')\n",
    "\n",
    "        image = np.load(image_file)\n",
    "        mask = np.load(mask_file)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            preds = outputs > 0.5\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_correct += preds.eq(masks).sum().item()\n",
    "            total_pixels += masks.numel()\n",
    "\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = total_correct / total_pixels\n",
    "    return average_loss, accuracy\n",
    "\n",
    "dataset_base = r'C:/Users/SirM/Desktop/Swayam/Intro to Deep Learning/Intro-to-Deep-Learning/Final Project/PKLot.v1-raw.yolov8-obb'\n",
    "partitions = ['train', 'valid', 'test']\n",
    "target_size = (256, 256)\n",
    "\n",
    "\n",
    "# Create datasets and dataloaders for train, validation, and test sets\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = PKLotDataset(os.path.join(dataset_base, 'train', 'images'), transform=transform)\n",
    "valid_dataset = PKLotDataset(os.path.join(dataset_base, 'valid', 'images'), transform=transform)\n",
    "test_dataset = PKLotDataset(os.path.join(dataset_base, 'test', 'images'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Setup device, model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SegmentationModel().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def calculate_accuracy(outputs, masks, threshold=0.5):\n",
    "    # Apply sigmoid to get probabilities\n",
    "    probs = torch.sigmoid(outputs)\n",
    "    # Apply threshold to get binary tensor\n",
    "    preds = (probs > threshold).float()\n",
    "    # Calculate accuracy\n",
    "    correct = (preds == masks).float()\n",
    "    accuracy = correct.sum() / correct.numel()\n",
    "    return accuracy.item()\n",
    "\n",
    "# Initialize lists to store accuracies for each batch\n",
    "accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    val_loss, val_accuracy = evaluate(model, valid_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'segmentation_model.pth')\n",
    "\n",
    "# Load the trained model for testing\n",
    "model.load_state_dict(torch.load('segmentation_model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "\n",
    "def visualize_predictions(model, loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > threshold).float()\n",
    "\n",
    "            # Move to CPU for visualization\n",
    "            images = images.cpu()\n",
    "            preds = preds.cpu()\n",
    "\n",
    "            plt.figure(figsize=(20, 20))\n",
    "            for idx in range(len(images)):\n",
    "                plt.subplot(len(images), 2, 2 * idx + 1)\n",
    "                plt.imshow(images[idx].permute(1, 2, 0))\n",
    "                plt.title(\"Image\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(len(images), 2, 2 * idx + 2)\n",
    "                plt.imshow(preds[idx].squeeze(), cmap='gray')\n",
    "                plt.title(\"Prediction\")\n",
    "                plt.axis('off')\n",
    "            plt.show()\n",
    "            break  # Only show 1 batch\n",
    "\n",
    "# Visualize the model's predictions on the test set\n",
    "visualize_predictions(model, test_loader, device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-27T20:15:37.083778600Z"
    }
   },
   "id": "8c912b5cbb17159a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [256, 128, 2, 2], expected input[8, 128, 11, 11] to have 256 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[74], line 169\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[0;32m    168\u001B[0m num_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m25\u001B[39m\n\u001B[1;32m--> 169\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate_accuracy\u001B[39m(model, dataloader, device):\n\u001B[0;32m    172\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n",
      "Cell \u001B[1;32mIn[74], line 156\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, dataloader, criterion, optimizer, num_epochs, device)\u001B[0m\n\u001B[0;32m    153\u001B[0m masks \u001B[38;5;241m=\u001B[39m masks\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()    \u001B[38;5;66;03m# Ensure masks are on GPU as float32\u001B[39;00m\n\u001B[0;32m    155\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 156\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, masks)\n\u001B[0;32m    158\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[74], line 126\u001B[0m, in \u001B[0;36mUNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    123\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool2(enc2)\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# ... continue forward pass\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;66;03m# Decoder\u001B[39;00m\n\u001B[1;32m--> 126\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mup1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x, enc2), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    128\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder1(x)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:952\u001B[0m, in \u001B[0;36mConvTranspose2d.forward\u001B[1;34m(self, input, output_size)\u001B[0m\n\u001B[0;32m    947\u001B[0m num_spatial_dims \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    948\u001B[0m output_padding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_padding(\n\u001B[0;32m    949\u001B[0m     \u001B[38;5;28minput\u001B[39m, output_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel_size,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    950\u001B[0m     num_spatial_dims, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m--> 952\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv_transpose2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    953\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    954\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_padding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Given transposed=1, weight of size [256, 128, 2, 2], expected input[8, 128, 11, 11] to have 256 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Set the base directory where the train, valid, and test folders are located\n",
    "base_dir = r'C:/Users/SirM/Downloads/PKLot.v1-raw.voc'\n",
    "\n",
    "# Define the desired image shape\n",
    "desired_shape = (45, 45)  # Example size, adjust as needed\n",
    "\n",
    "# Function to parse XML annotation and create a binary mask\n",
    "def parse_annotation(xml_path, img_shape):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Initialize a mask with zeros\n",
    "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    # Extract bounding boxes and set corresponding mask regions to 1\n",
    "    for obj in root.findall('object'):\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "        mask[ymin:ymax, xmin:xmax] = 1  # Mark occupied spaces as 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Function to load images and corresponding annotations\n",
    "def load_images_and_masks(data_dir, desired_shape):\n",
    "    image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        # Load image\n",
    "        img_path = os.path.join(data_dir, image_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, desired_shape[::-1])  # OpenCV expects width x height\n",
    "        img = img / 255.0  # Normalize image pixels to [0, 1]\n",
    "        \n",
    "        # Load corresponding annotation\n",
    "        xml_file = image_file.replace('.jpg', '.xml')\n",
    "        xml_path = os.path.join(data_dir, xml_file)\n",
    "        mask = parse_annotation(xml_path, img.shape)\n",
    "        mask = cv2.resize(mask, desired_shape[::-1])  # Resize mask like the image\n",
    "        \n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "        \n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Preprocess each dataset\n",
    "train_images, train_masks = load_images_and_masks(os.path.join(base_dir, 'train'), desired_shape)\n",
    "valid_images, valid_masks = load_images_and_masks(os.path.join(base_dir, 'valid'), desired_shape)\n",
    "test_images, test_masks = load_images_and_masks(os.path.join(base_dir, 'test'), desired_shape)\n",
    "\n",
    "class ParkingLotDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def transform(self, image, mask):\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask).float()\n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        mask = self.masks[index]\n",
    "        \n",
    "        image, mask = self.transform(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def transform(self, image, mask):\n",
    "        image = TF.to_tensor(image).float()  # Convert image to float32 tensor\n",
    "        mask = TF.to_tensor(mask).float()    # Convert mask to float32 tensor\n",
    "        return image, mask\n",
    "    \n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.encoder2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        # ... add more layers as needed\n",
    "        self.decoder1 = DoubleConv(128, 64)\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        # ... add more layers as needed\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        x = self.pool1(enc1)\n",
    "        enc2 = self.encoder2(x)\n",
    "        x = self.pool2(enc2)\n",
    "        # ... continue forward pass\n",
    "        # Decoder\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat((x, enc2), dim=1)\n",
    "        x = self.decoder1(x)\n",
    "        # ... continue forward pass\n",
    "        x = self.final_conv(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "model = UNet().to(device)\n",
    "\n",
    "# Create Dataset and DataLoader for train and validation sets\n",
    "train_dataset = ParkingLotDataset(train_images, train_masks)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "valid_dataset = ParkingLotDataset(valid_images, valid_masks)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Setup the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device).float()  # Ensure images are on GPU as float32\n",
    "            masks = masks.to(device).float()    # Ensure masks are on GPU as float32\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Print epoch loss\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader)}')\n",
    "\n",
    "\n",
    "# Start training\n",
    "num_epochs = 25\n",
    "train(model, train_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = outputs > 0.5\n",
    "            total += masks.numel()\n",
    "            correct += (predicted == masks).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Calculate accuracy on validation set\n",
    "accuracy = evaluate_accuracy(model, valid_loader,device)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T16:05:10.255982700Z",
     "start_time": "2024-04-27T16:00:03.213745900Z"
    }
   },
   "id": "e12bb127baebe1bb",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7e51d4cb1d682a14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
