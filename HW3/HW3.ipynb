{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1A. 10 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed9c24149ad70ca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.538947820663452, Validation Loss: 2.482860803604126, Validation Accuracy: 0.32773110270500183\n",
      "Epoch 20, Loss: 2.029510259628296, Validation Loss: 2.1493940353393555, Validation Accuracy: 0.4264705777168274\n",
      "Epoch 30, Loss: 1.6401114463806152, Validation Loss: 1.9770076274871826, Validation Accuracy: 0.47478991746902466\n",
      "Epoch 40, Loss: 1.2970045804977417, Validation Loss: 1.8995535373687744, Validation Accuracy: 0.49159663915634155\n",
      "Epoch 50, Loss: 0.9919137954711914, Validation Loss: 1.8426530361175537, Validation Accuracy: 0.5189075469970703\n",
      "Epoch 60, Loss: 0.7165911793708801, Validation Loss: 1.8662142753601074, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 70, Loss: 0.4867504835128784, Validation Loss: 1.9188909530639648, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 80, Loss: 0.30940911173820496, Validation Loss: 2.0147786140441895, Validation Accuracy: 0.506302535533905\n",
      "Epoch 90, Loss: 0.18833713233470917, Validation Loss: 2.1203010082244873, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 100, Loss: 0.11931469291448593, Validation Loss: 2.243370532989502, Validation Accuracy: 0.5084033608436584\n",
      "Predicted next character: 'a'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample text\n",
    "text =(\"Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting \"\n",
    "       \"the next character in a sequence of text based on the characters that precede it. This task is essential for various applications, \"\n",
    "       \"including text auto-completion, spell checking, and even in the development of sophisticated AI models capable of generating human-like text. \"\n",
    "       \"At its core, next character prediction relies on statistical models or deep learning algorithms to analyze a given sequence of text and \"\n",
    "       \"predict which character is most likely to follow. These predictions are based on patterns and relationships learned from large datasets of \"\n",
    "       \"text during the training phase of the model. One of the most popular approaches to next character prediction involves the use of Recurrent \"\n",
    "       \"Neural Networks (RNNs), and more specifically, a variant called Long Short-Term Memory (LSTM) networks. RNNs are particularly well-suited for\"\n",
    "       \" sequential data like text, as they can maintain information in 'memory' about previous characters to inform the prediction of the next character.\"\n",
    "       \" LSTM networks enhance this capability by being able to remember long-term dependencies, making them even more effective for next character \"\n",
    "       \"prediction tasks. Training a model for next character prediction involves feeding it large amounts of text data, allowing it to learn the \"\n",
    "       \"probability of each character's appearance following a sequence of characters. During this training process, the model adjusts its parameters \"\n",
    "       \"to minimize the difference between its predictions and the actual outcomes, thus improving its predictive accuracy over time. Once trained, \"\n",
    "       \"the model can be used to predict the next character in a given piece of text by considering the sequence of characters that precede it. \"\n",
    "       \"This can enhance user experience in text editing software, improve efficiency in coding environments with auto-completion features, and \"\n",
    "       \"enable more natural interactions with AI-based chatbots and virtual assistants. In summary, next character prediction plays a crucial role in \"\n",
    "       \"enhancing the capabilities of various NLP applications, making text-based interactions more efficient, accurate, and human-like. Through the use\"\n",
    "       \" of advanced machine learning models like RNNs and LSTMs, next character prediction continues to evolve, opening new possibilities for the future \"\n",
    "       \"of text-based technology.\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)} \n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the dataset\n",
    "max_length = 10  # Maximum length of input sequences\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(text) - max_length):\n",
    "    sequence = text[i:i + max_length]\n",
    "    label = text[i + max_length]\n",
    "    X.append([char_to_ix[char] for char in sequence])\n",
    "    y.append(char_to_ix[label])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Defining the RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
    "        return output\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.gru(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "learning_rate = 0.005\n",
    "epochs = 100\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T17:41:17.365412400Z",
     "start_time": "2024-03-05T17:40:55.234806300Z"
    }
   },
   "id": "4d8121cf105e365c",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7458ebde8c906e4e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5436341762542725, Validation Loss: 2.4962899684906006, Validation Accuracy: 0.32983192801475525\n",
      "Epoch 20, Loss: 2.044827461242676, Validation Loss: 2.1697237491607666, Validation Accuracy: 0.42016807198524475\n",
      "Epoch 30, Loss: 1.6352908611297607, Validation Loss: 1.9800901412963867, Validation Accuracy: 0.47478991746902466\n",
      "Epoch 40, Loss: 1.2673521041870117, Validation Loss: 1.897784948348999, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 50, Loss: 0.9275962710380554, Validation Loss: 1.9047890901565552, Validation Accuracy: 0.5252100825309753\n",
      "Epoch 60, Loss: 0.6253383159637451, Validation Loss: 1.9581834077835083, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 70, Loss: 0.38950324058532715, Validation Loss: 2.0432615280151367, Validation Accuracy: 0.506302535533905\n",
      "Epoch 80, Loss: 0.22826512157917023, Validation Loss: 2.129739999771118, Validation Accuracy: 0.4978991448879242\n",
      "Epoch 90, Loss: 0.1372307986021042, Validation Loss: 2.242154359817505, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 100, Loss: 0.09244585037231445, Validation Loss: 2.3462600708007812, Validation Accuracy: 0.5084033608436584\n",
      "Predicted next character: 'f'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T17:42:05.487899400Z",
     "start_time": "2024-03-05T17:41:33.768593600Z"
    }
   },
   "id": "1adcc743df382355",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7633bdfc1678a801"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.488182544708252, Validation Loss: 2.4525768756866455, Validation Accuracy: 0.3403361439704895\n",
      "Epoch 20, Loss: 1.9660340547561646, Validation Loss: 2.118163585662842, Validation Accuracy: 0.4264705777168274\n",
      "Epoch 30, Loss: 1.568363070487976, Validation Loss: 1.974371075630188, Validation Accuracy: 0.4831932783126831\n",
      "Epoch 40, Loss: 1.20578134059906, Validation Loss: 1.8996363878250122, Validation Accuracy: 0.5\n",
      "Epoch 50, Loss: 0.8849911689758301, Validation Loss: 1.8792941570281982, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 60, Loss: 0.604091465473175, Validation Loss: 1.941532015800476, Validation Accuracy: 0.5315126180648804\n",
      "Epoch 70, Loss: 0.38006526231765747, Validation Loss: 2.0621864795684814, Validation Accuracy: 0.5084033608436584\n",
      "Epoch 80, Loss: 0.22453683614730835, Validation Loss: 2.1934454441070557, Validation Accuracy: 0.506302535533905\n",
      "Epoch 90, Loss: 0.13568560779094696, Validation Loss: 2.3115172386169434, Validation Accuracy: 0.506302535533905\n",
      "Epoch 100, Loss: 0.09133424609899521, Validation Loss: 2.4238314628601074, Validation Accuracy: 0.4978991448879242\n",
      "Predicted next character: 'm'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T17:42:29.781283Z",
     "start_time": "2024-03-05T17:42:05.476897800Z"
    }
   },
   "id": "28c202379a64db14",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "1O LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a81e32b51f7a5f87"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.532536029815674, Validation Loss: 2.4758641719818115, Validation Accuracy: 0.31512606143951416\n",
      "Epoch 20, Loss: 2.0218567848205566, Validation Loss: 2.1617367267608643, Validation Accuracy: 0.4327731132507324\n",
      "Epoch 30, Loss: 1.6215181350708008, Validation Loss: 2.008790969848633, Validation Accuracy: 0.45798319578170776\n",
      "Epoch 40, Loss: 1.2695313692092896, Validation Loss: 1.9236398935317993, Validation Accuracy: 0.48739495873451233\n",
      "Epoch 50, Loss: 0.9612050652503967, Validation Loss: 1.9155203104019165, Validation Accuracy: 0.48739495873451233\n",
      "Epoch 60, Loss: 0.6820419430732727, Validation Loss: 1.928607702255249, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 70, Loss: 0.4530436396598816, Validation Loss: 1.9936124086380005, Validation Accuracy: 0.5189075469970703\n",
      "Epoch 80, Loss: 0.28237447142601013, Validation Loss: 2.087482213973999, Validation Accuracy: 0.5189075469970703\n",
      "Epoch 90, Loss: 0.17366564273834229, Validation Loss: 2.1961309909820557, Validation Accuracy: 0.5189075469970703\n",
      "Epoch 100, Loss: 0.11215485632419586, Validation Loss: 2.3101227283477783, Validation Accuracy: 0.5189075469970703\n",
      "Predicted next character: 'a'\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T17:43:24.691926300Z",
     "start_time": "2024-03-05T17:43:06.342425600Z"
    }
   },
   "id": "3752303c90f8f94f",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828197376341eae8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5772600173950195, Validation Loss: 2.5070526599884033, Validation Accuracy: 0.325630247592926\n",
      "Epoch 20, Loss: 2.0863804817199707, Validation Loss: 2.199014902114868, Validation Accuracy: 0.4285714328289032\n",
      "Epoch 30, Loss: 1.6980786323547363, Validation Loss: 2.032346248626709, Validation Accuracy: 0.4432772994041443\n",
      "Epoch 40, Loss: 1.3486571311950684, Validation Loss: 1.936607003211975, Validation Accuracy: 0.4852941036224365\n",
      "Epoch 50, Loss: 1.019872784614563, Validation Loss: 1.9033173322677612, Validation Accuracy: 0.4810924232006073\n",
      "Epoch 60, Loss: 0.7257246375083923, Validation Loss: 1.9383385181427002, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 70, Loss: 0.48039859533309937, Validation Loss: 2.0262200832366943, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 80, Loss: 0.2987724840641022, Validation Loss: 2.120030164718628, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 90, Loss: 0.1826467365026474, Validation Loss: 2.2285513877868652, Validation Accuracy: 0.49159663915634155\n",
      "Epoch 100, Loss: 0.1186407133936882, Validation Loss: 2.3202450275421143, Validation Accuracy: 0.4852941036224365\n",
      "Predicted next character: 't'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T17:44:18.404740100Z",
     "start_time": "2024-03-05T17:43:55.822110300Z"
    }
   },
   "id": "45cd952c440e20dc",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80d9f22ce726a5ae"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.542482852935791, Validation Loss: 2.4931578636169434, Validation Accuracy: 0.3445378243923187\n",
      "Epoch 20, Loss: 2.0392627716064453, Validation Loss: 2.1558632850646973, Validation Accuracy: 0.41806721687316895\n",
      "Epoch 30, Loss: 1.65337073802948, Validation Loss: 1.9960606098175049, Validation Accuracy: 0.4600840210914612\n",
      "Epoch 40, Loss: 1.3040956258773804, Validation Loss: 1.9099016189575195, Validation Accuracy: 0.5084033608436584\n",
      "Epoch 50, Loss: 0.9864965081214905, Validation Loss: 1.8779065608978271, Validation Accuracy: 0.5252100825309753\n",
      "Epoch 60, Loss: 0.7067068219184875, Validation Loss: 1.9204643964767456, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 70, Loss: 0.4737590551376343, Validation Loss: 1.9726073741912842, Validation Accuracy: 0.5252100825309753\n",
      "Epoch 80, Loss: 0.3022885322570801, Validation Loss: 2.0723044872283936, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 90, Loss: 0.19074365496635437, Validation Loss: 2.1873555183410645, Validation Accuracy: 0.506302535533905\n",
      "Epoch 100, Loss: 0.12517809867858887, Validation Loss: 2.301058053970337, Validation Accuracy: 0.48949578404426575\n",
      "Predicted next character: 'c'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T17:45:20.281399100Z",
     "start_time": "2024-03-05T17:44:58.714220600Z"
    }
   },
   "id": "2c0b8794ac7802d",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "10 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69461898f7980be4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.3777806758880615, Validation Loss: 2.3752284049987793, Validation Accuracy: 0.3781512677669525\n",
      "Epoch 20, Loss: 1.8709954023361206, Validation Loss: 2.1087470054626465, Validation Accuracy: 0.43487393856048584\n",
      "Epoch 30, Loss: 1.4659206867218018, Validation Loss: 1.9687341451644897, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 40, Loss: 1.1053141355514526, Validation Loss: 1.8806748390197754, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 50, Loss: 0.7785309553146362, Validation Loss: 1.8826634883880615, Validation Accuracy: 0.5315126180648804\n",
      "Epoch 60, Loss: 0.5035293698310852, Validation Loss: 1.9526100158691406, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 70, Loss: 0.2990594506263733, Validation Loss: 2.073343515396118, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 80, Loss: 0.17187370359897614, Validation Loss: 2.2228550910949707, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 90, Loss: 0.10461628437042236, Validation Loss: 2.3675670623779297, Validation Accuracy: 0.4978991448879242\n",
      "Epoch 100, Loss: 0.07393582910299301, Validation Loss: 2.473763942718506, Validation Accuracy: 0.5021008253097534\n",
      "Predicted next character: 'a'\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T18:05:12.296556400Z",
     "start_time": "2024-03-05T18:04:33.091695100Z"
    }
   },
   "id": "97a6de3670604e90",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c746900562d94d55"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.4077155590057373, Validation Loss: 2.391218900680542, Validation Accuracy: 0.3403361439704895\n",
      "Epoch 20, Loss: 1.8928576707839966, Validation Loss: 2.0913000106811523, Validation Accuracy: 0.45378151535987854\n",
      "Epoch 30, Loss: 1.4780752658843994, Validation Loss: 1.9101999998092651, Validation Accuracy: 0.49159663915634155\n",
      "Epoch 40, Loss: 1.1139681339263916, Validation Loss: 1.850730299949646, Validation Accuracy: 0.529411792755127\n",
      "Epoch 50, Loss: 0.7867543697357178, Validation Loss: 1.8516132831573486, Validation Accuracy: 0.5462185144424438\n",
      "Epoch 60, Loss: 0.5065733194351196, Validation Loss: 1.9372183084487915, Validation Accuracy: 0.5357142686843872\n",
      "Epoch 70, Loss: 0.2967461049556732, Validation Loss: 2.0838730335235596, Validation Accuracy: 0.5399159789085388\n",
      "Epoch 80, Loss: 0.16768525540828705, Validation Loss: 2.2396023273468018, Validation Accuracy: 0.5525209903717041\n",
      "Epoch 90, Loss: 0.10197245329618454, Validation Loss: 2.371074914932251, Validation Accuracy: 0.5420168042182922\n",
      "Epoch 100, Loss: 0.0721481665968895, Validation Loss: 2.4692928791046143, Validation Accuracy: 0.5420168042182922\n",
      "Predicted next character: 'a'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T18:05:45.524980Z",
     "start_time": "2024-03-05T18:05:12.280145900Z"
    }
   },
   "id": "86075b705d6e61e3",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "824877bd7bb566e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.3838353157043457, Validation Loss: 2.3537120819091797, Validation Accuracy: 0.3214285671710968\n",
      "Epoch 20, Loss: 1.849308729171753, Validation Loss: 2.054267406463623, Validation Accuracy: 0.4432772994041443\n",
      "Epoch 30, Loss: 1.4371122121810913, Validation Loss: 1.8946009874343872, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 40, Loss: 1.069371223449707, Validation Loss: 1.8227763175964355, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 50, Loss: 0.7419759035110474, Validation Loss: 1.8512152433395386, Validation Accuracy: 0.529411792755127\n",
      "Epoch 60, Loss: 0.47285255789756775, Validation Loss: 1.9473810195922852, Validation Accuracy: 0.5462185144424438\n",
      "Epoch 70, Loss: 0.2747008204460144, Validation Loss: 2.0933473110198975, Validation Accuracy: 0.5357142686843872\n",
      "Epoch 80, Loss: 0.15243537724018097, Validation Loss: 2.2739641666412354, Validation Accuracy: 0.5273109078407288\n",
      "Epoch 90, Loss: 0.09358999878168106, Validation Loss: 2.410339832305908, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 100, Loss: 0.06824540346860886, Validation Loss: 2.5178749561309814, Validation Accuracy: 0.5231092572212219\n",
      "Predicted next character: 'a'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is \"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T18:06:18.931829700Z",
     "start_time": "2024-03-05T18:05:45.510973700Z"
    }
   },
   "id": "65c01dac38d6f54a",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 20"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa8a74773fe1fd83"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 154\u001B[0m\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    153\u001B[0m     model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m--> 154\u001B[0m     \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;66;03m# Note: You need to adjust this code to add validation, adjust hyperparameters, and change sequence lengths.\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[20], line 143\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, epochs, lr)\u001B[0m\n\u001B[0;32m    141\u001B[0m y_pred, hidden \u001B[38;5;241m=\u001B[39m model(x, hidden)\n\u001B[0;32m    142\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(y_pred, y\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m--> 143\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    145\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "dataset_path = 'tiny_shakespeare.txt'\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a mapping for char to index and index to char\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "    idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "    \n",
    "    # Convert all text to integers\n",
    "    data = [char_to_idx[ch] for ch in text]\n",
    "    return data, char_to_idx, idx_to_char\n",
    "\n",
    "data, char_to_idx, idx_to_char = load_dataset(dataset_path)\n",
    "vocab_size = len(char_to_idx)  # Update vocab_size based on the actual dataset\n",
    "\n",
    "# Now you can define your CharDataset class and proceed with the rest of the setup\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.data[index:index+self.seq_length]), \n",
    "                torch.tensor(self.data[index+1:index+self.seq_length+1]))\n",
    "\n",
    "# Example: Creating the dataset with a sequence length\n",
    "seq_length = 30  # Or any other sequence length you want to experiment with\n",
    "dataset = CharDataset(data, seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a mapping for char to index and index to char\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "    idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "    \n",
    "    # Convert all text to integers\n",
    "    data = [char_to_idx[ch] for ch in text]\n",
    "    return data, char_to_idx, idx_to_char\n",
    "\n",
    "data, char_to_idx, idx_to_char = load_dataset(dataset_path)\n",
    "vocab_size = len(char_to_idx)  # Update vocab_size based on the actu\n",
    "\n",
    "# Example: Creating the dataset with a sequence length\n",
    "seq_length = 30  # Or any other sequence length you want to experiment with\n",
    "dataset = CharDataset(data, seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Assuming the dataset is already tokenized and available as a list of integers\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.data[index:index+self.seq_length]), \n",
    "                torch.tensor(self.data[index+1:index+self.seq_length+1]))\n",
    "\n",
    "# Model Definitions\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, rnn_type=\"LSTM\"):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:  # Default to GRU\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out.reshape(-1, self.hidden_dim))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            return (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                    weight.new(self.num_layers, batch_size, self.hidden_dim).zero_())\n",
    "        else:\n",
    "            return weight.new(self.num_layers, batch_size, self.hidden_dim).zero_()\n",
    "\n",
    "# Assuming `data` is your dataset loaded and processed into a list of tokenized integer values\n",
    "vocab_size = 65  # Example vocab size, adjust based on your dataset\n",
    "seq_length = 30  # Starting with sequence length of 30\n",
    "batch_size = 64\n",
    "dataset = CharDataset(data, seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "models = {\n",
    "    \"LSTM\": RNNModel(vocab_size, embed_dim, hidden_dim, num_layers, \"LSTM\"),\n",
    "    \"GRU\": RNNModel(vocab_size, embed_dim, hidden_dim, num_layers, \"GRU\")\n",
    "}\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, epochs, lr):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        total_loss = 0\n",
    "\n",
    "        for x, y in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred, hidden = model(x, hidden)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}, Time: {time.time() - start_time}s')\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model\")\n",
    "    model.to(device)\n",
    "    train_model(model, epochs, lr)\n",
    "\n",
    "# Note: You need to adjust this code to add validation, adjust hyperparameters, and change sequence lengths.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T19:54:34.613305400Z",
     "start_time": "2024-03-05T19:54:29.129306100Z"
    }
   },
   "id": "723e0b102cfe8cb0",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a35681c90107f8c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
