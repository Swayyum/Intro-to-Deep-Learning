{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1A. 10 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed9c24149ad70ca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5319173336029053, Validation Loss: 2.4783518314361572, Validation Accuracy: 0.32773110270500183\n",
      "Epoch 20, Loss: 2.0361487865448, Validation Loss: 2.1607296466827393, Validation Accuracy: 0.4264705777168274\n",
      "Epoch 30, Loss: 1.6470569372177124, Validation Loss: 1.9801265001296997, Validation Accuracy: 0.4642857015132904\n",
      "Epoch 40, Loss: 1.3060485124588013, Validation Loss: 1.890960931777954, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 50, Loss: 0.9892225861549377, Validation Loss: 1.8876657485961914, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 60, Loss: 0.695297360420227, Validation Loss: 1.9205610752105713, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 70, Loss: 0.4556044638156891, Validation Loss: 2.0270628929138184, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 80, Loss: 0.2768668830394745, Validation Loss: 2.1524016857147217, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 90, Loss: 0.16505244374275208, Validation Loss: 2.2903645038604736, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 100, Loss: 0.10549009591341019, Validation Loss: 2.4155712127685547, Validation Accuracy: 0.48949578404426575\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample text\n",
    "text =(\"Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting \"\n",
    "       \"the next character in a sequence of text based on the characters that precede it. This task is essential for various applications, \"\n",
    "       \"including text auto-completion, spell checking, and even in the development of sophisticated AI models capable of generating human-like text. \"\n",
    "       \"At its core, next character prediction relies on statistical models or deep learning algorithms to analyze a given sequence of text and \"\n",
    "       \"predict which character is most likely to follow. These predictions are based on patterns and relationships learned from large datasets of \"\n",
    "       \"text during the training phase of the model. One of the most popular approaches to next character prediction involves the use of Recurrent \"\n",
    "       \"Neural Networks (RNNs), and more specifically, a variant called Long Short-Term Memory (LSTM) networks. RNNs are particularly well-suited for\"\n",
    "       \" sequential data like text, as they can maintain information in 'memory' about previous characters to inform the prediction of the next character.\"\n",
    "       \" LSTM networks enhance this capability by being able to remember long-term dependencies, making them even more effective for next character \"\n",
    "       \"prediction tasks. Training a model for next character prediction involves feeding it large amounts of text data, allowing it to learn the \"\n",
    "       \"probability of each character's appearance following a sequence of characters. During this training process, the model adjusts its parameters \"\n",
    "       \"to minimize the difference between its predictions and the actual outcomes, thus improving its predictive accuracy over time. Once trained, \"\n",
    "       \"the model can be used to predict the next character in a given piece of text by considering the sequence of characters that precede it. \"\n",
    "       \"This can enhance user experience in text editing software, improve efficiency in coding environments with auto-completion features, and \"\n",
    "       \"enable more natural interactions with AI-based chatbots and virtual assistants. In summary, next character prediction plays a crucial role in \"\n",
    "       \"enhancing the capabilities of various NLP applications, making text-based interactions more efficient, accurate, and human-like. Through the use\"\n",
    "       \" of advanced machine learning models like RNNs and LSTMs, next character prediction continues to evolve, opening new possibilities for the future \"\n",
    "       \"of text-based technology.\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)} \n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the dataset\n",
    "max_length = 10  # Maximum length of input sequences\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(text) - max_length):\n",
    "    sequence = text[i:i + max_length]\n",
    "    label = text[i + max_length]\n",
    "    X.append([char_to_ix[char] for char in sequence])\n",
    "    y.append(char_to_ix[label])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Defining the RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
    "        return output\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.gru(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "learning_rate = 0.005\n",
    "epochs = 100\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:50:51.957044100Z",
     "start_time": "2024-03-08T03:49:57.362230800Z"
    }
   },
   "id": "4d8121cf105e365c",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7458ebde8c906e4e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5220096111297607, Validation Loss: 2.4657649993896484, Validation Accuracy: 0.3340336084365845\n",
      "Epoch 20, Loss: 2.031263589859009, Validation Loss: 2.1571600437164307, Validation Accuracy: 0.3949579894542694\n",
      "Epoch 30, Loss: 1.6530259847640991, Validation Loss: 2.0119218826293945, Validation Accuracy: 0.43907561898231506\n",
      "Epoch 40, Loss: 1.302440881729126, Validation Loss: 1.9274914264678955, Validation Accuracy: 0.4768907427787781\n",
      "Epoch 50, Loss: 0.9809191823005676, Validation Loss: 1.8938294649124146, Validation Accuracy: 0.4978991448879242\n",
      "Epoch 60, Loss: 0.6974117159843445, Validation Loss: 1.9417282342910767, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 70, Loss: 0.45580536127090454, Validation Loss: 1.992605209350586, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 80, Loss: 0.2755407989025116, Validation Loss: 2.0931529998779297, Validation Accuracy: 0.4852941036224365\n",
      "Epoch 90, Loss: 0.16291145980358124, Validation Loss: 2.197254180908203, Validation Accuracy: 0.49369746446609497\n",
      "Epoch 100, Loss: 0.10486096143722534, Validation Loss: 2.2961177825927734, Validation Accuracy: 0.4852941036224365\n",
      "Predicted next character: 'a'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:51:22.054649700Z",
     "start_time": "2024-03-08T03:50:51.966790200Z"
    }
   },
   "id": "1adcc743df382355",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7633bdfc1678a801"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5338222980499268, Validation Loss: 2.468808650970459, Validation Accuracy: 0.3172268867492676\n",
      "Epoch 20, Loss: 2.047011375427246, Validation Loss: 2.1640560626983643, Validation Accuracy: 0.4264705777168274\n",
      "Epoch 30, Loss: 1.6739567518234253, Validation Loss: 1.994498372077942, Validation Accuracy: 0.47268906235694885\n",
      "Epoch 40, Loss: 1.3376351594924927, Validation Loss: 1.914099097251892, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 50, Loss: 1.0287061929702759, Validation Loss: 1.8982856273651123, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 60, Loss: 0.7455872297286987, Validation Loss: 1.9173685312271118, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 70, Loss: 0.49708855152130127, Validation Loss: 2.003119707107544, Validation Accuracy: 0.5084033608436584\n",
      "Epoch 80, Loss: 0.30718210339546204, Validation Loss: 2.1226227283477783, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 90, Loss: 0.18527762591838837, Validation Loss: 2.219557046890259, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 100, Loss: 0.11866621673107147, Validation Loss: 2.3049681186676025, Validation Accuracy: 0.5126050710678101\n",
      "Predicted next character: 'l'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:51:55.661110800Z",
     "start_time": "2024-03-08T03:51:22.067458800Z"
    }
   },
   "id": "28c202379a64db14",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "1O LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a81e32b51f7a5f87"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.520794630050659, Validation Loss: 2.4925127029418945, Validation Accuracy: 0.34663864970207214\n",
      "Epoch 20, Loss: 2.016547203063965, Validation Loss: 2.1818032264709473, Validation Accuracy: 0.43907561898231506\n",
      "Epoch 30, Loss: 1.6090964078903198, Validation Loss: 2.009507417678833, Validation Accuracy: 0.45168066024780273\n",
      "Epoch 40, Loss: 1.2487033605575562, Validation Loss: 1.915102481842041, Validation Accuracy: 0.4978991448879242\n",
      "Epoch 50, Loss: 0.9222983121871948, Validation Loss: 1.8958710432052612, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 60, Loss: 0.6343286037445068, Validation Loss: 1.9344927072525024, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 70, Loss: 0.4052569568157196, Validation Loss: 2.006352663040161, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 80, Loss: 0.2449502795934677, Validation Loss: 2.1200826168060303, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 90, Loss: 0.1507578194141388, Validation Loss: 2.216162919998169, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 100, Loss: 0.10254059731960297, Validation Loss: 2.2977147102355957, Validation Accuracy: 0.5147058963775635\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:52:25.586188100Z",
     "start_time": "2024-03-08T03:51:55.663263900Z"
    }
   },
   "id": "3752303c90f8f94f",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828197376341eae8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.549938440322876, Validation Loss: 2.504688024520874, Validation Accuracy: 0.3445378243923187\n",
      "Epoch 20, Loss: 2.032592535018921, Validation Loss: 2.1786019802093506, Validation Accuracy: 0.424369752407074\n",
      "Epoch 30, Loss: 1.6261953115463257, Validation Loss: 1.9955672025680542, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.2634762525558472, Validation Loss: 1.913388967514038, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 50, Loss: 0.9457004070281982, Validation Loss: 1.9174299240112305, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 60, Loss: 0.6652361154556274, Validation Loss: 1.9478936195373535, Validation Accuracy: 0.5252100825309753\n",
      "Epoch 70, Loss: 0.43496087193489075, Validation Loss: 2.0106894969940186, Validation Accuracy: 0.5273109078407288\n",
      "Epoch 80, Loss: 0.2653399407863617, Validation Loss: 2.1706900596618652, Validation Accuracy: 0.506302535533905\n",
      "Epoch 90, Loss: 0.1608719676733017, Validation Loss: 2.318385362625122, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 100, Loss: 0.10625837743282318, Validation Loss: 2.429715156555176, Validation Accuracy: 0.48949578404426575\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:53:20.214336300Z",
     "start_time": "2024-03-08T03:52:25.600577100Z"
    }
   },
   "id": "45cd952c440e20dc",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80d9f22ce726a5ae"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5018138885498047, Validation Loss: 2.4373841285705566, Validation Accuracy: 0.3445378243923187\n",
      "Epoch 20, Loss: 2.00844144821167, Validation Loss: 2.1338062286376953, Validation Accuracy: 0.4306722581386566\n",
      "Epoch 30, Loss: 1.6190096139907837, Validation Loss: 1.9776421785354614, Validation Accuracy: 0.4474789798259735\n",
      "Epoch 40, Loss: 1.2626203298568726, Validation Loss: 1.8995037078857422, Validation Accuracy: 0.4852941036224365\n",
      "Epoch 50, Loss: 0.9395723938941956, Validation Loss: 1.906023383140564, Validation Accuracy: 0.506302535533905\n",
      "Epoch 60, Loss: 0.6515686511993408, Validation Loss: 1.9822111129760742, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 70, Loss: 0.41137897968292236, Validation Loss: 2.0669515132904053, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 80, Loss: 0.24650317430496216, Validation Loss: 2.1952030658721924, Validation Accuracy: 0.4810924232006073\n",
      "Epoch 90, Loss: 0.14683128893375397, Validation Loss: 2.3495113849639893, Validation Accuracy: 0.46848738193511963\n",
      "Epoch 100, Loss: 0.09726151823997498, Validation Loss: 2.4872615337371826, Validation Accuracy: 0.47058823704719543\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:54:04.711276Z",
     "start_time": "2024-03-08T03:53:20.243830700Z"
    }
   },
   "id": "2c0b8794ac7802d",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "10 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69461898f7980be4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.395632266998291, Validation Loss: 2.400958299636841, Validation Accuracy: 0.3424369692802429\n",
      "Epoch 20, Loss: 1.8846566677093506, Validation Loss: 2.089996337890625, Validation Accuracy: 0.4432772994041443\n",
      "Epoch 30, Loss: 1.4680081605911255, Validation Loss: 1.9066189527511597, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.0945087671279907, Validation Loss: 1.8267385959625244, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 50, Loss: 0.7577113509178162, Validation Loss: 1.8545242547988892, Validation Accuracy: 0.5273109078407288\n",
      "Epoch 60, Loss: 0.4758239686489105, Validation Loss: 1.945866584777832, Validation Accuracy: 0.529411792755127\n",
      "Epoch 70, Loss: 0.2701752781867981, Validation Loss: 2.0990848541259766, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 80, Loss: 0.14842531085014343, Validation Loss: 2.2591664791107178, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 90, Loss: 0.0918559655547142, Validation Loss: 2.3851773738861084, Validation Accuracy: 0.5\n",
      "Epoch 100, Loss: 0.06802092492580414, Validation Loss: 2.489683151245117, Validation Accuracy: 0.5210084319114685\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:54:42.574295100Z",
     "start_time": "2024-03-08T03:54:04.728016500Z"
    }
   },
   "id": "97a6de3670604e90",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c746900562d94d55"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.3771276473999023, Validation Loss: 2.3742356300354004, Validation Accuracy: 0.36764705181121826\n",
      "Epoch 20, Loss: 1.8542933464050293, Validation Loss: 2.0918853282928467, Validation Accuracy: 0.43487393856048584\n",
      "Epoch 30, Loss: 1.437756896018982, Validation Loss: 1.9142112731933594, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.054316759109497, Validation Loss: 1.8693901300430298, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 50, Loss: 0.7123831510543823, Validation Loss: 1.906084418296814, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 60, Loss: 0.4355407655239105, Validation Loss: 2.002800941467285, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 70, Loss: 0.2421967089176178, Validation Loss: 2.148542881011963, Validation Accuracy: 0.5189075469970703\n",
      "Epoch 80, Loss: 0.1324925720691681, Validation Loss: 2.299184560775757, Validation Accuracy: 0.5084033608436584\n",
      "Epoch 90, Loss: 0.08318573236465454, Validation Loss: 2.4269232749938965, Validation Accuracy: 0.5315126180648804\n",
      "Epoch 100, Loss: 0.06299571692943573, Validation Loss: 2.5357272624969482, Validation Accuracy: 0.5105041861534119\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:55:08.417960900Z",
     "start_time": "2024-03-08T03:54:42.576516400Z"
    }
   },
   "id": "86075b705d6e61e3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "824877bd7bb566e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.3970115184783936, Validation Loss: 2.3555400371551514, Validation Accuracy: 0.3571428656578064\n",
      "Epoch 20, Loss: 1.8836690187454224, Validation Loss: 2.068542003631592, Validation Accuracy: 0.45168066024780273\n",
      "Epoch 30, Loss: 1.4683642387390137, Validation Loss: 1.8898626565933228, Validation Accuracy: 0.48739495873451233\n",
      "Epoch 40, Loss: 1.0928021669387817, Validation Loss: 1.8155921697616577, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 50, Loss: 0.7567638754844666, Validation Loss: 1.8360581398010254, Validation Accuracy: 0.5462185144424438\n",
      "Epoch 60, Loss: 0.4811756908893585, Validation Loss: 1.9101457595825195, Validation Accuracy: 0.5420168042182922\n",
      "Epoch 70, Loss: 0.28263720870018005, Validation Loss: 2.027892827987671, Validation Accuracy: 0.5315126180648804\n",
      "Epoch 80, Loss: 0.15859577059745789, Validation Loss: 2.160409450531006, Validation Accuracy: 0.5357142686843872\n",
      "Epoch 90, Loss: 0.09645137935876846, Validation Loss: 2.2762715816497803, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 100, Loss: 0.06967311352491379, Validation Loss: 2.370497226715088, Validation Accuracy: 0.5378151535987854\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:55:37.387301700Z",
     "start_time": "2024-03-08T03:55:08.423446100Z"
    }
   },
   "id": "65c01dac38d6f54a",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 20"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa8a74773fe1fd83"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 154\u001B[0m\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    153\u001B[0m     model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m--> 154\u001B[0m     \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;66;03m# Note: You need to adjust this code to add validation, adjust hyperparameters, and change sequence lengths.\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[10], line 143\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, epochs, lr)\u001B[0m\n\u001B[0;32m    141\u001B[0m y_pred, hidden \u001B[38;5;241m=\u001B[39m model(x, hidden)\n\u001B[0;32m    142\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(y_pred, y\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m--> 143\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    145\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "dataset_path = 'tiny_shakespeare.txt'\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a mapping for char to index and index to char\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "    idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "    \n",
    "    # Convert all text to integers\n",
    "    data = [char_to_idx[ch] for ch in text]\n",
    "    return data, char_to_idx, idx_to_char\n",
    "\n",
    "data, char_to_idx, idx_to_char = load_dataset(dataset_path)\n",
    "vocab_size = len(char_to_idx)  # Update vocab_size based on the actual dataset\n",
    "\n",
    "# Now you can define your CharDataset class and proceed with the rest of the setup\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.data[index:index+self.seq_length]), \n",
    "                torch.tensor(self.data[index+1:index+self.seq_length+1]))\n",
    "\n",
    "# Example: Creating the dataset with a sequence length\n",
    "seq_length = 30  # Or any other sequence length you want to experiment with\n",
    "dataset = CharDataset(data, seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(dataset_path):\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a mapping for char to index and index to char\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "    idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "    \n",
    "    # Convert all text to integers\n",
    "    data = [char_to_idx[ch] for ch in text]\n",
    "    return data, char_to_idx, idx_to_char\n",
    "\n",
    "data, char_to_idx, idx_to_char = load_dataset(dataset_path)\n",
    "vocab_size = len(char_to_idx)  # Update vocab_size based on the actu\n",
    "\n",
    "# Example: Creating the dataset with a sequence length\n",
    "seq_length = 30  # Or any other sequence length you want to experiment with\n",
    "dataset = CharDataset(data, seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Assuming the dataset is already tokenized and available as a list of integers\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.data[index:index+self.seq_length]), \n",
    "                torch.tensor(self.data[index+1:index+self.seq_length+1]))\n",
    "\n",
    "# Model Definitions\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, rnn_type=\"LSTM\"):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        else:  # Default to GRU\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out.reshape(-1, self.hidden_dim))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            return (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                    weight.new(self.num_layers, batch_size, self.hidden_dim).zero_())\n",
    "        else:\n",
    "            return weight.new(self.num_layers, batch_size, self.hidden_dim).zero_()\n",
    "\n",
    "# Assuming `data` is your dataset loaded and processed into a list of tokenized integer values\n",
    "vocab_size = 65  # Example vocab size, adjust based on your dataset\n",
    "seq_length = 30  # Starting with sequence length of 30\n",
    "batch_size = 64\n",
    "dataset = CharDataset(data, seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "models = {\n",
    "    \"LSTM\": RNNModel(vocab_size, embed_dim, hidden_dim, num_layers, \"LSTM\"),\n",
    "    \"GRU\": RNNModel(vocab_size, embed_dim, hidden_dim, num_layers, \"GRU\")\n",
    "}\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, epochs, lr):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        total_loss = 0\n",
    "\n",
    "        for x, y in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred, hidden = model(x, hidden)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}, Time: {time.time() - start_time}s')\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model\")\n",
    "    model.to(device)\n",
    "    train_model(model, epochs, lr)\n",
    "\n",
    "# Note: You need to adjust this code to add validation, adjust hyperparameters, and change sequence lengths.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:55:41.247332100Z",
     "start_time": "2024-03-08T03:55:37.427062300Z"
    }
   },
   "id": "723e0b102cfe8cb0",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-08T03:55:41.243223800Z"
    }
   },
   "id": "a35681c90107f8c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
