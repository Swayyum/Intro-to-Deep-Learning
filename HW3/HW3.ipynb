{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1A. 10 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed9c24149ad70ca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tiny_shakespeare.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 116\u001B[0m\n\u001B[0;32m    113\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m    114\u001B[0m lr \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.002\u001B[39m\n\u001B[1;32m--> 116\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m data_loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    119\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[12], line 89\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m(sequence_length)\u001B[0m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data\u001B[39m(sequence_length):\n\u001B[1;32m---> 89\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtiny_shakespeare.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     90\u001B[0m         text \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ShakespeareDataset(text, sequence_length)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'tiny_shakespeare.txt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample text\n",
    "text =(\"Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting \"\n",
    "       \"the next character in a sequence of text based on the characters that precede it. This task is essential for various applications, \"\n",
    "       \"including text auto-completion, spell checking, and even in the development of sophisticated AI models capable of generating human-like text. \"\n",
    "       \"At its core, next character prediction relies on statistical models or deep learning algorithms to analyze a given sequence of text and \"\n",
    "       \"predict which character is most likely to follow. These predictions are based on patterns and relationships learned from large datasets of \"\n",
    "       \"text during the training phase of the model. One of the most popular approaches to next character prediction involves the use of Recurrent \"\n",
    "       \"Neural Networks (RNNs), and more specifically, a variant called Long Short-Term Memory (LSTM) networks. RNNs are particularly well-suited for\"\n",
    "       \" sequential data like text, as they can maintain information in 'memory' about previous characters to inform the prediction of the next character.\"\n",
    "       \" LSTM networks enhance this capability by being able to remember long-term dependencies, making them even more effective for next character \"\n",
    "       \"prediction tasks. Training a model for next character prediction involves feeding it large amounts of text data, allowing it to learn the \"\n",
    "       \"probability of each character's appearance following a sequence of characters. During this training process, the model adjusts its parameters \"\n",
    "       \"to minimize the difference between its predictions and the actual outcomes, thus improving its predictive accuracy over time. Once trained, \"\n",
    "       \"the model can be used to predict the next character in a given piece of text by considering the sequence of characters that precede it. \"\n",
    "       \"This can enhance user experience in text editing software, improve efficiency in coding environments with auto-completion features, and \"\n",
    "       \"enable more natural interactions with AI-based chatbots and virtual assistants. In summary, next character prediction plays a crucial role in \"\n",
    "       \"enhancing the capabilities of various NLP applications, making text-based interactions more efficient, accurate, and human-like. Through the use\"\n",
    "       \" of advanced machine learning models like RNNs and LSTMs, next character prediction continues to evolve, opening new possibilities for the future \"\n",
    "       \"of text-based technology.\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)} \n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the dataset\n",
    "max_length = 10  # Maximum length of input sequences\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(text) - max_length):\n",
    "    sequence = text[i:i + max_length]\n",
    "    label = text[i + max_length]\n",
    "    X.append([char_to_ix[char] for char in sequence])\n",
    "    y.append(char_to_ix[label])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Defining the RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
    "        return output\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.gru(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "learning_rate = 0.005\n",
    "epochs = 100\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T00:04:02.167643700Z",
     "start_time": "2024-03-10T00:04:02.034000100Z"
    }
   },
   "id": "4d8121cf105e365c",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7458ebde8c906e4e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5220096111297607, Validation Loss: 2.4657649993896484, Validation Accuracy: 0.3340336084365845\n",
      "Epoch 20, Loss: 2.031263589859009, Validation Loss: 2.1571600437164307, Validation Accuracy: 0.3949579894542694\n",
      "Epoch 30, Loss: 1.6530259847640991, Validation Loss: 2.0119218826293945, Validation Accuracy: 0.43907561898231506\n",
      "Epoch 40, Loss: 1.302440881729126, Validation Loss: 1.9274914264678955, Validation Accuracy: 0.4768907427787781\n",
      "Epoch 50, Loss: 0.9809191823005676, Validation Loss: 1.8938294649124146, Validation Accuracy: 0.4978991448879242\n",
      "Epoch 60, Loss: 0.6974117159843445, Validation Loss: 1.9417282342910767, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 70, Loss: 0.45580536127090454, Validation Loss: 1.992605209350586, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 80, Loss: 0.2755407989025116, Validation Loss: 2.0931529998779297, Validation Accuracy: 0.4852941036224365\n",
      "Epoch 90, Loss: 0.16291145980358124, Validation Loss: 2.197254180908203, Validation Accuracy: 0.49369746446609497\n",
      "Epoch 100, Loss: 0.10486096143722534, Validation Loss: 2.2961177825927734, Validation Accuracy: 0.4852941036224365\n",
      "Predicted next character: 'a'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:51:22.054649700Z",
     "start_time": "2024-03-08T03:50:51.966790200Z"
    }
   },
   "id": "1adcc743df382355",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7633bdfc1678a801"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5338222980499268, Validation Loss: 2.468808650970459, Validation Accuracy: 0.3172268867492676\n",
      "Epoch 20, Loss: 2.047011375427246, Validation Loss: 2.1640560626983643, Validation Accuracy: 0.4264705777168274\n",
      "Epoch 30, Loss: 1.6739567518234253, Validation Loss: 1.994498372077942, Validation Accuracy: 0.47268906235694885\n",
      "Epoch 40, Loss: 1.3376351594924927, Validation Loss: 1.914099097251892, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 50, Loss: 1.0287061929702759, Validation Loss: 1.8982856273651123, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 60, Loss: 0.7455872297286987, Validation Loss: 1.9173685312271118, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 70, Loss: 0.49708855152130127, Validation Loss: 2.003119707107544, Validation Accuracy: 0.5084033608436584\n",
      "Epoch 80, Loss: 0.30718210339546204, Validation Loss: 2.1226227283477783, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 90, Loss: 0.18527762591838837, Validation Loss: 2.219557046890259, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 100, Loss: 0.11866621673107147, Validation Loss: 2.3049681186676025, Validation Accuracy: 0.5126050710678101\n",
      "Predicted next character: 'l'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:51:55.661110800Z",
     "start_time": "2024-03-08T03:51:22.067458800Z"
    }
   },
   "id": "28c202379a64db14",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "1O LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a81e32b51f7a5f87"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.520794630050659, Validation Loss: 2.4925127029418945, Validation Accuracy: 0.34663864970207214\n",
      "Epoch 20, Loss: 2.016547203063965, Validation Loss: 2.1818032264709473, Validation Accuracy: 0.43907561898231506\n",
      "Epoch 30, Loss: 1.6090964078903198, Validation Loss: 2.009507417678833, Validation Accuracy: 0.45168066024780273\n",
      "Epoch 40, Loss: 1.2487033605575562, Validation Loss: 1.915102481842041, Validation Accuracy: 0.4978991448879242\n",
      "Epoch 50, Loss: 0.9222983121871948, Validation Loss: 1.8958710432052612, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 60, Loss: 0.6343286037445068, Validation Loss: 1.9344927072525024, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 70, Loss: 0.4052569568157196, Validation Loss: 2.006352663040161, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 80, Loss: 0.2449502795934677, Validation Loss: 2.1200826168060303, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 90, Loss: 0.1507578194141388, Validation Loss: 2.216162919998169, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 100, Loss: 0.10254059731960297, Validation Loss: 2.2977147102355957, Validation Accuracy: 0.5147058963775635\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:52:25.586188100Z",
     "start_time": "2024-03-08T03:51:55.663263900Z"
    }
   },
   "id": "3752303c90f8f94f",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828197376341eae8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.549938440322876, Validation Loss: 2.504688024520874, Validation Accuracy: 0.3445378243923187\n",
      "Epoch 20, Loss: 2.032592535018921, Validation Loss: 2.1786019802093506, Validation Accuracy: 0.424369752407074\n",
      "Epoch 30, Loss: 1.6261953115463257, Validation Loss: 1.9955672025680542, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.2634762525558472, Validation Loss: 1.913388967514038, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 50, Loss: 0.9457004070281982, Validation Loss: 1.9174299240112305, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 60, Loss: 0.6652361154556274, Validation Loss: 1.9478936195373535, Validation Accuracy: 0.5252100825309753\n",
      "Epoch 70, Loss: 0.43496087193489075, Validation Loss: 2.0106894969940186, Validation Accuracy: 0.5273109078407288\n",
      "Epoch 80, Loss: 0.2653399407863617, Validation Loss: 2.1706900596618652, Validation Accuracy: 0.506302535533905\n",
      "Epoch 90, Loss: 0.1608719676733017, Validation Loss: 2.318385362625122, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 100, Loss: 0.10625837743282318, Validation Loss: 2.429715156555176, Validation Accuracy: 0.48949578404426575\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:53:20.214336300Z",
     "start_time": "2024-03-08T03:52:25.600577100Z"
    }
   },
   "id": "45cd952c440e20dc",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80d9f22ce726a5ae"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5018138885498047, Validation Loss: 2.4373841285705566, Validation Accuracy: 0.3445378243923187\n",
      "Epoch 20, Loss: 2.00844144821167, Validation Loss: 2.1338062286376953, Validation Accuracy: 0.4306722581386566\n",
      "Epoch 30, Loss: 1.6190096139907837, Validation Loss: 1.9776421785354614, Validation Accuracy: 0.4474789798259735\n",
      "Epoch 40, Loss: 1.2626203298568726, Validation Loss: 1.8995037078857422, Validation Accuracy: 0.4852941036224365\n",
      "Epoch 50, Loss: 0.9395723938941956, Validation Loss: 1.906023383140564, Validation Accuracy: 0.506302535533905\n",
      "Epoch 60, Loss: 0.6515686511993408, Validation Loss: 1.9822111129760742, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 70, Loss: 0.41137897968292236, Validation Loss: 2.0669515132904053, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 80, Loss: 0.24650317430496216, Validation Loss: 2.1952030658721924, Validation Accuracy: 0.4810924232006073\n",
      "Epoch 90, Loss: 0.14683128893375397, Validation Loss: 2.3495113849639893, Validation Accuracy: 0.46848738193511963\n",
      "Epoch 100, Loss: 0.09726151823997498, Validation Loss: 2.4872615337371826, Validation Accuracy: 0.47058823704719543\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:54:04.711276Z",
     "start_time": "2024-03-08T03:53:20.243830700Z"
    }
   },
   "id": "2c0b8794ac7802d",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "10 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69461898f7980be4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.395632266998291, Validation Loss: 2.400958299636841, Validation Accuracy: 0.3424369692802429\n",
      "Epoch 20, Loss: 1.8846566677093506, Validation Loss: 2.089996337890625, Validation Accuracy: 0.4432772994041443\n",
      "Epoch 30, Loss: 1.4680081605911255, Validation Loss: 1.9066189527511597, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.0945087671279907, Validation Loss: 1.8267385959625244, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 50, Loss: 0.7577113509178162, Validation Loss: 1.8545242547988892, Validation Accuracy: 0.5273109078407288\n",
      "Epoch 60, Loss: 0.4758239686489105, Validation Loss: 1.945866584777832, Validation Accuracy: 0.529411792755127\n",
      "Epoch 70, Loss: 0.2701752781867981, Validation Loss: 2.0990848541259766, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 80, Loss: 0.14842531085014343, Validation Loss: 2.2591664791107178, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 90, Loss: 0.0918559655547142, Validation Loss: 2.3851773738861084, Validation Accuracy: 0.5\n",
      "Epoch 100, Loss: 0.06802092492580414, Validation Loss: 2.489683151245117, Validation Accuracy: 0.5210084319114685\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:54:42.574295100Z",
     "start_time": "2024-03-08T03:54:04.728016500Z"
    }
   },
   "id": "97a6de3670604e90",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c746900562d94d55"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.3771276473999023, Validation Loss: 2.3742356300354004, Validation Accuracy: 0.36764705181121826\n",
      "Epoch 20, Loss: 1.8542933464050293, Validation Loss: 2.0918853282928467, Validation Accuracy: 0.43487393856048584\n",
      "Epoch 30, Loss: 1.437756896018982, Validation Loss: 1.9142112731933594, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.054316759109497, Validation Loss: 1.8693901300430298, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 50, Loss: 0.7123831510543823, Validation Loss: 1.906084418296814, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 60, Loss: 0.4355407655239105, Validation Loss: 2.002800941467285, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 70, Loss: 0.2421967089176178, Validation Loss: 2.148542881011963, Validation Accuracy: 0.5189075469970703\n",
      "Epoch 80, Loss: 0.1324925720691681, Validation Loss: 2.299184560775757, Validation Accuracy: 0.5084033608436584\n",
      "Epoch 90, Loss: 0.08318573236465454, Validation Loss: 2.4269232749938965, Validation Accuracy: 0.5315126180648804\n",
      "Epoch 100, Loss: 0.06299571692943573, Validation Loss: 2.5357272624969482, Validation Accuracy: 0.5105041861534119\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:55:08.417960900Z",
     "start_time": "2024-03-08T03:54:42.576516400Z"
    }
   },
   "id": "86075b705d6e61e3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "824877bd7bb566e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.3970115184783936, Validation Loss: 2.3555400371551514, Validation Accuracy: 0.3571428656578064\n",
      "Epoch 20, Loss: 1.8836690187454224, Validation Loss: 2.068542003631592, Validation Accuracy: 0.45168066024780273\n",
      "Epoch 30, Loss: 1.4683642387390137, Validation Loss: 1.8898626565933228, Validation Accuracy: 0.48739495873451233\n",
      "Epoch 40, Loss: 1.0928021669387817, Validation Loss: 1.8155921697616577, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 50, Loss: 0.7567638754844666, Validation Loss: 1.8360581398010254, Validation Accuracy: 0.5462185144424438\n",
      "Epoch 60, Loss: 0.4811756908893585, Validation Loss: 1.9101457595825195, Validation Accuracy: 0.5420168042182922\n",
      "Epoch 70, Loss: 0.28263720870018005, Validation Loss: 2.027892827987671, Validation Accuracy: 0.5315126180648804\n",
      "Epoch 80, Loss: 0.15859577059745789, Validation Loss: 2.160409450531006, Validation Accuracy: 0.5357142686843872\n",
      "Epoch 90, Loss: 0.09645137935876846, Validation Loss: 2.2762715816497803, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 100, Loss: 0.06967311352491379, Validation Loss: 2.370497226715088, Validation Accuracy: 0.5378151535987854\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:55:37.387301700Z",
     "start_time": "2024-03-08T03:55:08.423446100Z"
    }
   },
   "id": "65c01dac38d6f54a",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 20"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa8a74773fe1fd83"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.7511989493446656, Train Accuracy: 48.15%, Val Loss: 1.5606276364553542, Val Accuracy: 52.76%\n",
      "Epoch 2, Train Loss: 1.5173584691628736, Train Accuracy: 53.95%, Val Loss: 1.4977184385659705, Val Accuracy: 54.61%\n",
      "Epoch 3, Train Loss: 1.4786085673980307, Train Accuracy: 54.92%, Val Loss: 1.4776417698862905, Val Accuracy: 54.99%\n",
      "Epoch 4, Train Loss: 1.4576729451932924, Train Accuracy: 55.33%, Val Loss: 1.4668254681312005, Val Accuracy: 54.96%\n",
      "Epoch 5, Train Loss: 1.444161927946258, Train Accuracy: 55.67%, Val Loss: 1.450158799782062, Val Accuracy: 55.70%\n",
      "Total Training Time: 978.80 seconds\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# LSTM Model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences_one_hot)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train_correct += (predicted == targets).sum().item()\n",
    "            total_train_samples += targets.size(0)\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "                outputs = model(sequences_one_hot)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val_correct += (predicted == targets).sum().item()\n",
    "                total_val_samples += targets.size(0)\n",
    "\n",
    "        val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, '\n",
    "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {total_val_loss / len(test_loader)}, '\n",
    "              f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    end_time = time.time()  # Record the end time of training\n",
    "    training_time = end_time - start_time  # Calculate the total training time\n",
    "    print(f'Total Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_int, int_to_char, initial_str, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = [char_to_int.get(c, 0) for c in initial_str[-max_length:]]  # Handle characters not in dict\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "        sequence_one_hot = torch.nn.functional.one_hot(sequence, num_classes=vocab_size).float()\n",
    "        prediction = model(sequence_one_hot)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return int_to_char[predicted_index]\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    sequence_length = 20  # Choose a sequence length\n",
    "    hidden_dim = 128  # Choose a hidden dimension size\n",
    "    num_layers = 2  # Choose number of LSTM layers\n",
    "    batch_size = 128  # Batch size for training\n",
    "    epochs = 5  # Number of epochs to train for\n",
    "\n",
    "    # Prepare the dataset\n",
    "    sequences, targets = [], []\n",
    "    for i in range(0, len(text) - sequence_length):\n",
    "        seq = [char_to_int[ch] for ch in text[i:i+sequence_length]]\n",
    "        target = char_to_int[text[i+sequence_length]]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(vocab_size, hidden_dim, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train and validate the model\n",
    "    train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n",
    "\n",
    "    # Prediction example after training\n",
    "    test_str = \"We are accounted poor citiz\"\n",
    "    predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str, sequence_length)\n",
    "    print(f\"Predicted next character: '{predicted_char}'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T02:07:54.683561Z",
     "start_time": "2024-03-10T01:51:30.422069500Z"
    }
   },
   "id": "723e0b102cfe8cb0",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.6962894519392404, Train Accuracy: 49.58%, Val Loss: 1.534514790899919, Val Accuracy: 53.48%\n",
      "Epoch 2, Train Loss: 1.4898458307868643, Train Accuracy: 54.68%, Val Loss: 1.4744970922967995, Val Accuracy: 54.98%\n",
      "Epoch 3, Train Loss: 1.4509233574859042, Train Accuracy: 55.65%, Val Loss: 1.4517270606000465, Val Accuracy: 55.58%\n",
      "Epoch 4, Train Loss: 1.428999951112496, Train Accuracy: 56.17%, Val Loss: 1.44063881013972, Val Accuracy: 55.91%\n",
      "Epoch 5, Train Loss: 1.4140226065052424, Train Accuracy: 56.55%, Val Loss: 1.430225047139142, Val Accuracy: 56.32%\n",
      "Total Training Time: 1531.09 seconds\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# LSTM Model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences_one_hot)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train_correct += (predicted == targets).sum().item()\n",
    "            total_train_samples += targets.size(0)\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "                outputs = model(sequences_one_hot)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val_correct += (predicted == targets).sum().item()\n",
    "                total_val_samples += targets.size(0)\n",
    "\n",
    "        val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, '\n",
    "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {total_val_loss / len(test_loader)}, '\n",
    "              f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    end_time = time.time()  # Record the end time of training\n",
    "    training_time = end_time - start_time  # Calculate the total training time\n",
    "    print(f'Total Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_int, int_to_char, initial_str, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = [char_to_int.get(c, 0) for c in initial_str[-max_length:]]  # Handle characters not in dict\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "        sequence_one_hot = torch.nn.functional.one_hot(sequence, num_classes=vocab_size).float()\n",
    "        prediction = model(sequence_one_hot)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return int_to_char[predicted_index]\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    sequence_length = 30  # Choose a sequence length\n",
    "    hidden_dim = 128  # Choose a hidden dimension size\n",
    "    num_layers = 2  # Choose number of LSTM layers\n",
    "    batch_size = 128  # Batch size for training\n",
    "    epochs = 5  # Number of epochs to train for\n",
    "\n",
    "    # Prepare the dataset\n",
    "    sequences, targets = [], []\n",
    "    for i in range(0, len(text) - sequence_length):\n",
    "        seq = [char_to_int[ch] for ch in text[i:i+sequence_length]]\n",
    "        target = char_to_int[text[i+sequence_length]]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(vocab_size, hidden_dim, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train and validate the model\n",
    "    train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n",
    "\n",
    "    # Prediction example after training\n",
    "    test_str = \"We are accounted poor citiz\"\n",
    "    predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str, sequence_length)\n",
    "    print(f\"Predicted next character: '{predicted_char}'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T02:33:32.180572300Z",
     "start_time": "2024-03-10T02:07:54.712399600Z"
    }
   },
   "id": "a35681c90107f8c6",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# LSTM Model class\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.lstm = nn.GRU(vocab_size, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        out, _ = self.lstm(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences_one_hot)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train_correct += (predicted == targets).sum().item()\n",
    "            total_train_samples += targets.size(0)\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "                outputs = model(sequences_one_hot)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val_correct += (predicted == targets).sum().item()\n",
    "                total_val_samples += targets.size(0)\n",
    "\n",
    "        val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, '\n",
    "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {total_val_loss / len(test_loader)}, '\n",
    "              f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    end_time = time.time()  # Record the end time of training\n",
    "    training_time = end_time - start_time  # Calculate the total training time\n",
    "    print(f'Total Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_int, int_to_char, initial_str, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = [char_to_int.get(c, 0) for c in initial_str[-max_length:]]  # Handle characters not in dict\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "        sequence_one_hot = torch.nn.functional.one_hot(sequence, num_classes=vocab_size).float()\n",
    "        prediction = model(sequence_one_hot)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return int_to_char[predicted_index]\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    sequence_length = 20  # Choose a sequence length\n",
    "    hidden_dim = 128  # Choose a hidden dimension size\n",
    "    num_layers = 2  # Choose number of LSTM layers\n",
    "    batch_size = 128  # Batch size for training\n",
    "    epochs = 5  # Number of epochs to train for\n",
    "\n",
    "    # Prepare the dataset\n",
    "    sequences, targets = [], []\n",
    "    for i in range(0, len(text) - sequence_length):\n",
    "        seq = [char_to_int[ch] for ch in text[i:i+sequence_length]]\n",
    "        target = char_to_int[text[i+sequence_length]]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GRUModel(vocab_size, hidden_dim, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train and validate the model\n",
    "    train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n",
    "\n",
    "    # Prediction example after training\n",
    "    test_str = \"We are accounted poor citiz\"\n",
    "    predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str, sequence_length)\n",
    "    print(f\"Predicted next character: '{predicted_char}'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ac217908a9b73ca4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# LSTM Model class\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.lstm = nn.GRU(vocab_size, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        out, _ = self.lstm(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences_one_hot)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train_correct += (predicted == targets).sum().item()\n",
    "            total_train_samples += targets.size(0)\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "                outputs = model(sequences_one_hot)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val_correct += (predicted == targets).sum().item()\n",
    "                total_val_samples += targets.size(0)\n",
    "\n",
    "        val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, '\n",
    "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {total_val_loss / len(test_loader)}, '\n",
    "              f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    end_time = time.time()  # Record the end time of training\n",
    "    training_time = end_time - start_time  # Calculate the total training time\n",
    "    print(f'Total Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_int, int_to_char, initial_str, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = [char_to_int.get(c, 0) for c in initial_str[-max_length:]]  # Handle characters not in dict\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "        sequence_one_hot = torch.nn.functional.one_hot(sequence, num_classes=vocab_size).float()\n",
    "        prediction = model(sequence_one_hot)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return int_to_char[predicted_index]\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    sequence_length = 30  # Choose a sequence length\n",
    "    hidden_dim = 128  # Choose a hidden dimension size\n",
    "    num_layers = 2  # Choose number of LSTM layers\n",
    "    batch_size = 128  # Batch size for training\n",
    "    epochs = 5  # Number of epochs to train for\n",
    "\n",
    "    # Prepare the dataset\n",
    "    sequences, targets = [], []\n",
    "    for i in range(0, len(text) - sequence_length):\n",
    "        seq = [char_to_int[ch] for ch in text[i:i+sequence_length]]\n",
    "        target = char_to_int[text[i+sequence_length]]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GRUModel(vocab_size, hidden_dim, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train and validate the model\n",
    "    train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n",
    "\n",
    "    # Prediction example after training\n",
    "    test_str = \"We are accounted poor citiz\"\n",
    "    predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str, sequence_length)\n",
    "    print(f\"Predicted next character: '{predicted_char}'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8517bd2314d25683"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# LSTM Model class\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.lstm = nn.GRU(vocab_size, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        out, _ = self.lstm(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences_one_hot)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train_correct += (predicted == targets).sum().item()\n",
    "            total_train_samples += targets.size(0)\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "                outputs = model(sequences_one_hot)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val_correct += (predicted == targets).sum().item()\n",
    "                total_val_samples += targets.size(0)\n",
    "\n",
    "        val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, '\n",
    "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {total_val_loss / len(test_loader)}, '\n",
    "              f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    end_time = time.time()  # Record the end time of training\n",
    "    training_time = end_time - start_time  # Calculate the total training time\n",
    "    print(f'Total Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_int, int_to_char, initial_str, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = [char_to_int.get(c, 0) for c in initial_str[-max_length:]]  # Handle characters not in dict\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "        sequence_one_hot = torch.nn.functional.one_hot(sequence, num_classes=vocab_size).float()\n",
    "        prediction = model(sequence_one_hot)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return int_to_char[predicted_index]\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    sequence_length = 50  # Choose a sequence length\n",
    "    hidden_dim = 128  # Choose a hidden dimension size\n",
    "    num_layers = 2  # Choose number of LSTM layers\n",
    "    batch_size = 128  # Batch size for training\n",
    "    epochs = 5  # Number of epochs to train for\n",
    "\n",
    "    # Prepare the dataset\n",
    "    sequences, targets = [], []\n",
    "    for i in range(0, len(text) - sequence_length):\n",
    "        seq = [char_to_int[ch] for ch in text[i:i+sequence_length]]\n",
    "        target = char_to_int[text[i+sequence_length]]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GRUModel(vocab_size, hidden_dim, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train and validate the model\n",
    "    train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n",
    "\n",
    "    # Prediction example after training\n",
    "    test_str = \"We are accounted poor citiz\"\n",
    "    predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str, sequence_length)\n",
    "    print(f\"Predicted next character: '{predicted_char}'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8a9d179584b8060d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# LSTM Model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_dim)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Function to train and validate the model\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences_one_hot)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train_correct += (predicted == targets).sum().item()\n",
    "            total_train_samples += targets.size(0)\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences_one_hot = torch.nn.functional.one_hot(sequences, num_classes=vocab_size).float()\n",
    "                outputs = model(sequences_one_hot)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val_correct += (predicted == targets).sum().item()\n",
    "                total_val_samples += targets.size(0)\n",
    "\n",
    "        val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, '\n",
    "              f'Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {total_val_loss / len(test_loader)}, '\n",
    "              f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    end_time = time.time()  # Record the end time of training\n",
    "    training_time = end_time - start_time  # Calculate the total training time\n",
    "    print(f'Total Training Time: {training_time:.2f} seconds')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_int, int_to_char, initial_str, max_length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sequence = [char_to_int.get(c, 0) for c in initial_str[-max_length:]]  # Handle characters not in dict\n",
    "        sequence = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)\n",
    "        sequence_one_hot = torch.nn.functional.one_hot(sequence, num_classes=vocab_size).float()\n",
    "        prediction = model(sequence_one_hot)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return int_to_char[predicted_index]\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    sequence_length = 50  # Choose a sequence length\n",
    "    hidden_dim = 128  # Choose a hidden dimension size\n",
    "    num_layers = 2  # Choose number of LSTM layers\n",
    "    batch_size = 128  # Batch size for training\n",
    "    epochs = 5  # Number of epochs to train for\n",
    "\n",
    "    # Prepare the dataset\n",
    "    sequences, targets = [], []\n",
    "    for i in range(0, len(text) - sequence_length):\n",
    "        seq = [char_to_int[ch] for ch in text[i:i+sequence_length]]\n",
    "        target = char_to_int[text[i+sequence_length]]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(vocab_size, hidden_dim, num_layers)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train and validate the model\n",
    "    train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=epochs)\n",
    "\n",
    "    # Prediction example after training\n",
    "    test_str = \"We are accounted poor citiz\"\n",
    "    predicted_char = predict_next_char(model, char_to_int, int_to_char, test_str, sequence_length)\n",
    "    print(f\"Predicted next character: '{predicted_char}'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "92446710f60a56e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f2dc89766ad8ed3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
