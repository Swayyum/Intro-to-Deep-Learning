{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1A. 10 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ed9c24149ad70ca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5319173336029053, Validation Loss: 2.4783518314361572, Validation Accuracy: 0.32773110270500183\n",
      "Epoch 20, Loss: 2.0361487865448, Validation Loss: 2.1607296466827393, Validation Accuracy: 0.4264705777168274\n",
      "Epoch 30, Loss: 1.6470569372177124, Validation Loss: 1.9801265001296997, Validation Accuracy: 0.4642857015132904\n",
      "Epoch 40, Loss: 1.3060485124588013, Validation Loss: 1.890960931777954, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 50, Loss: 0.9892225861549377, Validation Loss: 1.8876657485961914, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 60, Loss: 0.695297360420227, Validation Loss: 1.9205610752105713, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 70, Loss: 0.4556044638156891, Validation Loss: 2.0270628929138184, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 80, Loss: 0.2768668830394745, Validation Loss: 2.1524016857147217, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 90, Loss: 0.16505244374275208, Validation Loss: 2.2903645038604736, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 100, Loss: 0.10549009591341019, Validation Loss: 2.4155712127685547, Validation Accuracy: 0.48949578404426575\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample text\n",
    "text =(\"Next character prediction is a fundamental task in the field of natural language processing (NLP) that involves predicting \"\n",
    "       \"the next character in a sequence of text based on the characters that precede it. This task is essential for various applications, \"\n",
    "       \"including text auto-completion, spell checking, and even in the development of sophisticated AI models capable of generating human-like text. \"\n",
    "       \"At its core, next character prediction relies on statistical models or deep learning algorithms to analyze a given sequence of text and \"\n",
    "       \"predict which character is most likely to follow. These predictions are based on patterns and relationships learned from large datasets of \"\n",
    "       \"text during the training phase of the model. One of the most popular approaches to next character prediction involves the use of Recurrent \"\n",
    "       \"Neural Networks (RNNs), and more specifically, a variant called Long Short-Term Memory (LSTM) networks. RNNs are particularly well-suited for\"\n",
    "       \" sequential data like text, as they can maintain information in 'memory' about previous characters to inform the prediction of the next character.\"\n",
    "       \" LSTM networks enhance this capability by being able to remember long-term dependencies, making them even more effective for next character \"\n",
    "       \"prediction tasks. Training a model for next character prediction involves feeding it large amounts of text data, allowing it to learn the \"\n",
    "       \"probability of each character's appearance following a sequence of characters. During this training process, the model adjusts its parameters \"\n",
    "       \"to minimize the difference between its predictions and the actual outcomes, thus improving its predictive accuracy over time. Once trained, \"\n",
    "       \"the model can be used to predict the next character in a given piece of text by considering the sequence of characters that precede it. \"\n",
    "       \"This can enhance user experience in text editing software, improve efficiency in coding environments with auto-completion features, and \"\n",
    "       \"enable more natural interactions with AI-based chatbots and virtual assistants. In summary, next character prediction plays a crucial role in \"\n",
    "       \"enhancing the capabilities of various NLP applications, making text-based interactions more efficient, accurate, and human-like. Through the use\"\n",
    "       \" of advanced machine learning models like RNNs and LSTMs, next character prediction continues to evolve, opening new possibilities for the future \"\n",
    "       \"of text-based technology.\")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)} \n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the dataset\n",
    "max_length = 10  # Maximum length of input sequences\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(text) - max_length):\n",
    "    sequence = text[i:i + max_length]\n",
    "    label = text[i + max_length]\n",
    "    X.append([char_to_ix[char] for char in sequence])\n",
    "    y.append(char_to_ix[label])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(X_val, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Defining the RNN model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])  # Get the output of the last RNN cell\n",
    "        return output\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.gru(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "learning_rate = 0.005\n",
    "epochs = 100\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:50:51.957044100Z",
     "start_time": "2024-03-08T03:49:57.362230800Z"
    }
   },
   "id": "4d8121cf105e365c",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7458ebde8c906e4e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5220096111297607, Validation Loss: 2.4657649993896484, Validation Accuracy: 0.3340336084365845\n",
      "Epoch 20, Loss: 2.031263589859009, Validation Loss: 2.1571600437164307, Validation Accuracy: 0.3949579894542694\n",
      "Epoch 30, Loss: 1.6530259847640991, Validation Loss: 2.0119218826293945, Validation Accuracy: 0.43907561898231506\n",
      "Epoch 40, Loss: 1.302440881729126, Validation Loss: 1.9274914264678955, Validation Accuracy: 0.4768907427787781\n",
      "Epoch 50, Loss: 0.9809191823005676, Validation Loss: 1.8938294649124146, Validation Accuracy: 0.4978991448879242\n",
      "Epoch 60, Loss: 0.6974117159843445, Validation Loss: 1.9417282342910767, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 70, Loss: 0.45580536127090454, Validation Loss: 1.992605209350586, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 80, Loss: 0.2755407989025116, Validation Loss: 2.0931529998779297, Validation Accuracy: 0.4852941036224365\n",
      "Epoch 90, Loss: 0.16291145980358124, Validation Loss: 2.197254180908203, Validation Accuracy: 0.49369746446609497\n",
      "Epoch 100, Loss: 0.10486096143722534, Validation Loss: 2.2961177825927734, Validation Accuracy: 0.4852941036224365\n",
      "Predicted next character: 'a'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:51:22.054649700Z",
     "start_time": "2024-03-08T03:50:51.966790200Z"
    }
   },
   "id": "1adcc743df382355",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7633bdfc1678a801"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5338222980499268, Validation Loss: 2.468808650970459, Validation Accuracy: 0.3172268867492676\n",
      "Epoch 20, Loss: 2.047011375427246, Validation Loss: 2.1640560626983643, Validation Accuracy: 0.4264705777168274\n",
      "Epoch 30, Loss: 1.6739567518234253, Validation Loss: 1.994498372077942, Validation Accuracy: 0.47268906235694885\n",
      "Epoch 40, Loss: 1.3376351594924927, Validation Loss: 1.914099097251892, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 50, Loss: 1.0287061929702759, Validation Loss: 1.8982856273651123, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 60, Loss: 0.7455872297286987, Validation Loss: 1.9173685312271118, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 70, Loss: 0.49708855152130127, Validation Loss: 2.003119707107544, Validation Accuracy: 0.5084033608436584\n",
      "Epoch 80, Loss: 0.30718210339546204, Validation Loss: 2.1226227283477783, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 90, Loss: 0.18527762591838837, Validation Loss: 2.219557046890259, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 100, Loss: 0.11866621673107147, Validation Loss: 2.3049681186676025, Validation Accuracy: 0.5126050710678101\n",
      "Predicted next character: 'l'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharRNN(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:51:55.661110800Z",
     "start_time": "2024-03-08T03:51:22.067458800Z"
    }
   },
   "id": "28c202379a64db14",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "1O LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a81e32b51f7a5f87"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.520794630050659, Validation Loss: 2.4925127029418945, Validation Accuracy: 0.34663864970207214\n",
      "Epoch 20, Loss: 2.016547203063965, Validation Loss: 2.1818032264709473, Validation Accuracy: 0.43907561898231506\n",
      "Epoch 30, Loss: 1.6090964078903198, Validation Loss: 2.009507417678833, Validation Accuracy: 0.45168066024780273\n",
      "Epoch 40, Loss: 1.2487033605575562, Validation Loss: 1.915102481842041, Validation Accuracy: 0.4978991448879242\n",
      "Epoch 50, Loss: 0.9222983121871948, Validation Loss: 1.8958710432052612, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 60, Loss: 0.6343286037445068, Validation Loss: 1.9344927072525024, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 70, Loss: 0.4052569568157196, Validation Loss: 2.006352663040161, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 80, Loss: 0.2449502795934677, Validation Loss: 2.1200826168060303, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 90, Loss: 0.1507578194141388, Validation Loss: 2.216162919998169, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 100, Loss: 0.10254059731960297, Validation Loss: 2.2977147102355957, Validation Accuracy: 0.5147058963775635\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:52:25.586188100Z",
     "start_time": "2024-03-08T03:51:55.663263900Z"
    }
   },
   "id": "3752303c90f8f94f",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828197376341eae8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.549938440322876, Validation Loss: 2.504688024520874, Validation Accuracy: 0.3445378243923187\n",
      "Epoch 20, Loss: 2.032592535018921, Validation Loss: 2.1786019802093506, Validation Accuracy: 0.424369752407074\n",
      "Epoch 30, Loss: 1.6261953115463257, Validation Loss: 1.9955672025680542, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.2634762525558472, Validation Loss: 1.913388967514038, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 50, Loss: 0.9457004070281982, Validation Loss: 1.9174299240112305, Validation Accuracy: 0.5210084319114685\n",
      "Epoch 60, Loss: 0.6652361154556274, Validation Loss: 1.9478936195373535, Validation Accuracy: 0.5252100825309753\n",
      "Epoch 70, Loss: 0.43496087193489075, Validation Loss: 2.0106894969940186, Validation Accuracy: 0.5273109078407288\n",
      "Epoch 80, Loss: 0.2653399407863617, Validation Loss: 2.1706900596618652, Validation Accuracy: 0.506302535533905\n",
      "Epoch 90, Loss: 0.1608719676733017, Validation Loss: 2.318385362625122, Validation Accuracy: 0.4957983195781708\n",
      "Epoch 100, Loss: 0.10625837743282318, Validation Loss: 2.429715156555176, Validation Accuracy: 0.48949578404426575\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:53:20.214336300Z",
     "start_time": "2024-03-08T03:52:25.600577100Z"
    }
   },
   "id": "45cd952c440e20dc",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80d9f22ce726a5ae"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.5018138885498047, Validation Loss: 2.4373841285705566, Validation Accuracy: 0.3445378243923187\n",
      "Epoch 20, Loss: 2.00844144821167, Validation Loss: 2.1338062286376953, Validation Accuracy: 0.4306722581386566\n",
      "Epoch 30, Loss: 1.6190096139907837, Validation Loss: 1.9776421785354614, Validation Accuracy: 0.4474789798259735\n",
      "Epoch 40, Loss: 1.2626203298568726, Validation Loss: 1.8995037078857422, Validation Accuracy: 0.4852941036224365\n",
      "Epoch 50, Loss: 0.9395723938941956, Validation Loss: 1.906023383140564, Validation Accuracy: 0.506302535533905\n",
      "Epoch 60, Loss: 0.6515686511993408, Validation Loss: 1.9822111129760742, Validation Accuracy: 0.5168067216873169\n",
      "Epoch 70, Loss: 0.41137897968292236, Validation Loss: 2.0669515132904053, Validation Accuracy: 0.48949578404426575\n",
      "Epoch 80, Loss: 0.24650317430496216, Validation Loss: 2.1952030658721924, Validation Accuracy: 0.4810924232006073\n",
      "Epoch 90, Loss: 0.14683128893375397, Validation Loss: 2.3495113849639893, Validation Accuracy: 0.46848738193511963\n",
      "Epoch 100, Loss: 0.09726151823997498, Validation Loss: 2.4872615337371826, Validation Accuracy: 0.47058823704719543\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharLSTM(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:54:04.711276Z",
     "start_time": "2024-03-08T03:53:20.243830700Z"
    }
   },
   "id": "2c0b8794ac7802d",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "10 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69461898f7980be4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.395632266998291, Validation Loss: 2.400958299636841, Validation Accuracy: 0.3424369692802429\n",
      "Epoch 20, Loss: 1.8846566677093506, Validation Loss: 2.089996337890625, Validation Accuracy: 0.4432772994041443\n",
      "Epoch 30, Loss: 1.4680081605911255, Validation Loss: 1.9066189527511597, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.0945087671279907, Validation Loss: 1.8267385959625244, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 50, Loss: 0.7577113509178162, Validation Loss: 1.8545242547988892, Validation Accuracy: 0.5273109078407288\n",
      "Epoch 60, Loss: 0.4758239686489105, Validation Loss: 1.945866584777832, Validation Accuracy: 0.529411792755127\n",
      "Epoch 70, Loss: 0.2701752781867981, Validation Loss: 2.0990848541259766, Validation Accuracy: 0.5147058963775635\n",
      "Epoch 80, Loss: 0.14842531085014343, Validation Loss: 2.2591664791107178, Validation Accuracy: 0.5126050710678101\n",
      "Epoch 90, Loss: 0.0918559655547142, Validation Loss: 2.3851773738861084, Validation Accuracy: 0.5\n",
      "Epoch 100, Loss: 0.06802092492580414, Validation Loss: 2.489683151245117, Validation Accuracy: 0.5210084319114685\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:54:42.574295100Z",
     "start_time": "2024-03-08T03:54:04.728016500Z"
    }
   },
   "id": "97a6de3670604e90",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "20 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c746900562d94d55"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.3771276473999023, Validation Loss: 2.3742356300354004, Validation Accuracy: 0.36764705181121826\n",
      "Epoch 20, Loss: 1.8542933464050293, Validation Loss: 2.0918853282928467, Validation Accuracy: 0.43487393856048584\n",
      "Epoch 30, Loss: 1.437756896018982, Validation Loss: 1.9142112731933594, Validation Accuracy: 0.5042017102241516\n",
      "Epoch 40, Loss: 1.054316759109497, Validation Loss: 1.8693901300430298, Validation Accuracy: 0.5105041861534119\n",
      "Epoch 50, Loss: 0.7123831510543823, Validation Loss: 1.906084418296814, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 60, Loss: 0.4355407655239105, Validation Loss: 2.002800941467285, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 70, Loss: 0.2421967089176178, Validation Loss: 2.148542881011963, Validation Accuracy: 0.5189075469970703\n",
      "Epoch 80, Loss: 0.1324925720691681, Validation Loss: 2.299184560775757, Validation Accuracy: 0.5084033608436584\n",
      "Epoch 90, Loss: 0.08318573236465454, Validation Loss: 2.4269232749938965, Validation Accuracy: 0.5315126180648804\n",
      "Epoch 100, Loss: 0.06299571692943573, Validation Loss: 2.5357272624969482, Validation Accuracy: 0.5105041861534119\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:55:08.417960900Z",
     "start_time": "2024-03-08T03:54:42.576516400Z"
    }
   },
   "id": "86075b705d6e61e3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "30 GRU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "824877bd7bb566e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.3970115184783936, Validation Loss: 2.3555400371551514, Validation Accuracy: 0.3571428656578064\n",
      "Epoch 20, Loss: 1.8836690187454224, Validation Loss: 2.068542003631592, Validation Accuracy: 0.45168066024780273\n",
      "Epoch 30, Loss: 1.4683642387390137, Validation Loss: 1.8898626565933228, Validation Accuracy: 0.48739495873451233\n",
      "Epoch 40, Loss: 1.0928021669387817, Validation Loss: 1.8155921697616577, Validation Accuracy: 0.5231092572212219\n",
      "Epoch 50, Loss: 0.7567638754844666, Validation Loss: 1.8360581398010254, Validation Accuracy: 0.5462185144424438\n",
      "Epoch 60, Loss: 0.4811756908893585, Validation Loss: 1.9101457595825195, Validation Accuracy: 0.5420168042182922\n",
      "Epoch 70, Loss: 0.28263720870018005, Validation Loss: 2.027892827987671, Validation Accuracy: 0.5315126180648804\n",
      "Epoch 80, Loss: 0.15859577059745789, Validation Loss: 2.160409450531006, Validation Accuracy: 0.5357142686843872\n",
      "Epoch 90, Loss: 0.09645137935876846, Validation Loss: 2.2762715816497803, Validation Accuracy: 0.5336134433746338\n",
      "Epoch 100, Loss: 0.06967311352491379, Validation Loss: 2.370497226715088, Validation Accuracy: 0.5378151535987854\n",
      "Predicted next character: 'e'\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "# Model, loss, and optimizer\n",
    "model = CharGRU(len(chars), hidden_size, len(chars))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val)\n",
    "        val_loss = criterion(val_output, y_val)\n",
    "        _, predicted = torch.max(val_output, 1)\n",
    "        val_accuracy = (predicted == y_val).float().mean()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy.item()}')\n",
    "\n",
    "# Prediction function\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0)\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "# Predicting the next character\n",
    "test_str = \"Next character prediction is a fundam\"\n",
    "predicted_char = predict_next_char(model, char_to_ix, ix_to_char, test_str)\n",
    "print(f\"Predicted next character: '{predicted_char}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T03:55:37.387301700Z",
     "start_time": "2024-03-08T03:55:08.423446100Z"
    }
   },
   "id": "65c01dac38d6f54a",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 20"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa8a74773fe1fd83"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with sequence_length=20, hidden_dim=64, num_layers=1\n",
      "Total Training Time: 114.87 seconds\n",
      "Epoch 1, Train Loss: 1.8320663928028091, Train Accuracy: 46.17%, Val Loss: 1.6760413726718002, Val Accuracy: 50.02%\n",
      "Total Training Time: 226.46 seconds\n",
      "Epoch 2, Train Loss: 1.6404405225643262, Train Accuracy: 50.95%, Val Loss: 1.62021776081431, Val Accuracy: 51.43%\n",
      "Total Training Time: 342.60 seconds\n",
      "Epoch 3, Train Loss: 1.6060598080895512, Train Accuracy: 51.87%, Val Loss: 1.6054406148978368, Val Accuracy: 51.98%\n",
      "Total Training Time: 453.75 seconds\n",
      "Epoch 4, Train Loss: 1.590910049429996, Train Accuracy: 52.27%, Val Loss: 1.5952073060023122, Val Accuracy: 52.31%\n",
      "Total Training Time: 564.90 seconds\n",
      "Epoch 5, Train Loss: 1.5830871723912874, Train Accuracy: 52.42%, Val Loss: 1.5871546001045853, Val Accuracy: 52.48%\n",
      "Model Complexity (Number of Trainable Parameters): 37761\n",
      "Predicted next character for sequence length 20: 'e'\n",
      "Training with sequence_length=20, hidden_dim=64, num_layers=2\n",
      "Total Training Time: 211.22 seconds\n",
      "Epoch 1, Train Loss: 1.886055421176069, Train Accuracy: 44.71%, Val Loss: 1.680528197901316, Val Accuracy: 49.76%\n",
      "Total Training Time: 392.87 seconds\n",
      "Epoch 2, Train Loss: 1.6394811484810954, Train Accuracy: 50.89%, Val Loss: 1.6044381976606377, Val Accuracy: 51.88%\n",
      "Total Training Time: 574.38 seconds\n",
      "Epoch 3, Train Loss: 1.5932286889198795, Train Accuracy: 52.02%, Val Loss: 1.593145349418303, Val Accuracy: 52.22%\n",
      "Total Training Time: 760.95 seconds\n",
      "Epoch 4, Train Loss: 1.5723213402108769, Train Accuracy: 52.62%, Val Loss: 1.576670324056913, Val Accuracy: 52.73%\n",
      "Total Training Time: 946.16 seconds\n",
      "Epoch 5, Train Loss: 1.5687629227418018, Train Accuracy: 52.76%, Val Loss: 1.822777380426246, Val Accuracy: 47.16%\n",
      "Model Complexity (Number of Trainable Parameters): 71041\n",
      "Predicted next character for sequence length 20: 'e'\n",
      "Training with sequence_length=20, hidden_dim=128, num_layers=1\n",
      "Total Training Time: 209.57 seconds\n",
      "Epoch 1, Train Loss: 1.7432608215445016, Train Accuracy: 48.31%, Val Loss: 1.5594326350720396, Val Accuracy: 52.90%\n",
      "Total Training Time: 525.24 seconds\n",
      "Epoch 2, Train Loss: 1.5276287923803702, Train Accuracy: 53.71%, Val Loss: 1.5165205519567326, Val Accuracy: 54.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, seq_length):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        self.sequences = [self.char_to_int[ch] for ch in text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.sequences[index:index+self.seq_length], dtype=torch.long),\n",
    "                torch.tensor(self.sequences[index+1:index+1+self.seq_length], dtype=torch.long))\n",
    "\n",
    "# RNN Model class for both LSTM and GRU\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, model_type=\"LSTM\"):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        if model_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif model_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        if self.model_type == \"LSTM\":\n",
    "            if hidden is None:\n",
    "                hidden = (torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device),\n",
    "                          torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device))\n",
    "            out, hidden = self.rnn(x, hidden)\n",
    "        elif self.model_type == \"GRU\":\n",
    "            if hidden is None:\n",
    "                hidden = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "            out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "def train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs=5, device=torch.device(\"cpu\")):\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(sequences)\n",
    "            loss = criterion(output.transpose(1, 2), targets)  # Adjust for the shape [batch_size, vocab_size, seq_length]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences, targets = sequences.to(device), targets.to(device)\n",
    "                output, _ = model(sequences)\n",
    "                val_loss = criterion(output.transpose(1, 2), targets)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        # Print Epoch results\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {total_loss / len(train_loader):.4f}, Validation Loss: {total_val_loss / len(test_loader):.4f}')\n",
    "\n",
    "    # Total training time\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'Total training time: {total_time:.2f} seconds\\n')\n",
    "\n",
    "# Hyperparameters & Training configurations\n",
    "sequence_length = 50\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = CharDataset(text, sequence_length)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "models = {\n",
    "    \"LSTM\": RNNModel(vocab_size, hidden_dim, num_layers, \"LSTM\"),\n",
    "    \"GRU\": RNNModel(vocab_size, hidden_dim, num_layers, \"GRU\")\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_and_validate(model, train_loader, test_loader, criterion, optimizer, epochs, device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-09T04:17:03.271490700Z"
    }
   },
   "id": "723e0b102cfe8cb0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-08T03:55:41.243223800Z"
    }
   },
   "id": "a35681c90107f8c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
